---
title: 如何选择深度强化学习算法
---

## 如何选择深度强化学习算法？MuZero/SAC/PPO/TD3/DDPG/DQN/等（已完成） - 曾伊言的文章 - 知乎
https://zhuanlan.zhihu.com/p/342919579
## 赶时间请直接看加粗的四种算法，它们占据不同的生态位，请根据实际任务需要去选择他们，在强化学习的子领域（多智能体、分层强化学习、逆向强化学习也会以它们为基础开发新的算法）：

离散动作空间推荐：Dueling DoubleQN（D3QN）
连续动作空间推荐：擅长调参就用TD3，不擅长调参就用PPO或SAC，如果训练环境 Reward function 都是初学者写的，那就用PPO
没入门深度强化学习的人请按顺序学习以下算法：

入门深度学习/机器学习，用多层全连接层跑一下 MNIST数据集
入门深度学习/深度学习框架，用卷积网络跑一下 MNIST-fashion数据集
入门经典强化学习 Q-learning，离散状态、离散动作
入门深度强化学习 DQN（Deep Q-learning），连续状态、离散动作
入门深度强化学习 DDPG（Deep DPG），连续状态，连续动作
用于入门的算法只能用来入门，实际做项目千万不要使用，至少也要用加粗的算法，尽管它们不是2021的 SotA（State of the Art 最高水准的算法），但已经足够好用，且不至于太复杂。一些性能卓越但是过于复杂的算法，我会写在下面。为了方便你们了解强化学习的子领域，我会列举突出研究成果，并写上短介绍。若能驾驭，就去用。可以不从头到尾地阅读本文，推荐的用法是：当成字典去「查」，请善用crtl +F。这几个算法的实现代码可以在这里找到：强化学习算法库「小雅：ElegantRL」。与本文配套的姊妹篇见：

曾伊言：深度强化学习调参技巧：以D3QN、TD3、PPO、SAC算法为例（有空再添加图片）
​
zhuanlan.zhihu.com
图标
目录
基础算法：

更新日志 第一版 2021-01-12，2021-1-26 姊妹篇，2021-1-27 Hybrid action，修正优势函数的来历，修正Dueling 的解释
部分无模型强化学习算法总结图片
离散的动作空间 discrete action space
连续的动作空间 continuous action space
混合的动作空间 hybrid action space
其他领域：

改进经验回放，以适应稀疏奖励 sparse reward
多智能体算法 MultiAgent RL
分层强化学习 Hierarchical RL
逆向强化学习 Inverse RL 与 模仿学习 Imitation Learning
基于模型的强化学习算法 Model-based RL（重点介绍MuZero）
补充知识：

好用的强化学习算法是？
学习曲线怎么看？
好的算法的学习曲线应该是？
交流：深度学习ElegantRL QQ群号 1163106809
交流：评论区的好问题会在这里回答，或者直接写入正文。
注意！我只熟悉无模型强化学习算法 model-free RL，因而其他领域有错请大家直接指出，不用客气。


部分无模型强化学习算法总结图片
高清图片见 ElegantRL - Model-free_DRL_2020.png


搜索图中的网址的加粗部分ElegantRL、LilianWeng、EwanLi 策略梯度 就能找到
下面我会按照这种格式去介绍算法：

这里会写上深度强化学习的子领域
XXX（算法的全称或别称）顾名思义地解释此算法，并补充其他特点，帮助使用者判断此算法适用领域。
XXX，大体上按照发表时间从早到晚排序，推荐按顺序了解排在前面的适合入门的算法，理解后，再使用靠后性能较好但复杂的算法。把连续的工作会被放在一起，如 DQN→DDQN→D3QN，按这些顺序去了解效率更高。
XXX，贡献明显的算法才有资格被我介绍（我不熟悉的领域的好算法可能被我漏掉，请指出）。那种只有A+B的低创新工作不会在此处被提及。例外：D3QN = DoubleDQN + DuelingDQN。D3QN是因为简单好用才上榜，不需要对应的论文。
XXX，我会少量地列举一些不可靠、不诚实的论文，并将我的质疑写出来。
离散的动作空间 discrete action space

月球登陆器 LunarLander-v2，离散动作空间
一个离散动作空间例子：月球登陆器LunarLander-v2。它有四个引擎，每次可以开启不同方向的引擎。安全平稳降落、消耗更少燃料都会让最终得分变高。环境每一次重置 reset 都会刷新飞行器的初速度，因此需要多测几次得到的平均分才能作为给定策略的得分。

DQN（Deep Q Network）可用于入门深度强化学习，使用一个Q Network来估计Q值，从而替换了 Q-table，完成从离散状态空间到连续状态空间的跨越。Q Network 会对每一个离散动作的Q值进行估计，执行的时候选择Q值最高的动作（greedy 策略）。并使用 epslion-greedy 策略进行探索（探索的时候，有很小的概率随机执行动作），来获得各种动作的训练数据。
DDQN（Double DQN）更加稳定，因为最优化操作会传播高估误差，所以她同时训练两个Q network并选择较小的Q值用于计算TD-error，降低高估误差。详见和DDQN一样采用了两个估值网络的TD3 曾伊言：强化学习算法TD3论文的翻译与解读
Dueling DQN，Dueling DQN 使用了优势函数 advantage function（A3C也用了）：它只估计state的Q值，不考虑动作，好的策略能将state 导向一个更有优势的局面。然而不是任何时刻 action 都会影响 state的转移，因此Dueling DQN 结合了 优势函数估计的Q值 与 原本DQN对不同动作估计的Q值。DQN算法学习 state 与每个离散动作一一对应的Q值后才能知道学到 state 的Q值，而Dueling DQN 能通过优势函数直接学到state的价值，这使得Dueling DQN在一些action不影响环境的情况下能学比DQN更快。
Intuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful in states where its actions do not affect the environment in any relevant way. ---- 来自 Dueling DQN论文 Introduction。
D3QN（Dueling Double DQN），Dueling DQN 与Double DQN相互兼容，一起用效果很好。简单，泛用，没有使用禁忌。任何一个刚入门的人都能独立地在前两种算法的基础上改出D3QN。在论文中使用了D3QN应该引用DuelingDQN 与 DoubleDQN的文章。
Noisy DQN，探索能力稍强。Noisy DQN 把噪声添加到网络的输出层之前值。原本Q值较大的动作在添加噪声后Q值变大的概率也比较大。这种探索比epslion-greedy随机选一个动作去执行更好，至少这种针对性的探索既保证了探索动作多样，也提高了探索效率。
====估计Q值的期望↑ ↓估计Q值的分布====

Distributional RL 值分布RL（C51，Distributional Perspective RL）。在DQN中，Q Network 拟合了Q值的期望，期望可以用一个数值去描述，比较简单。在值分布DQN中，Q Network 拟合了Q值的分布，Q值分布的描述就要麻烦一些了，但是训练效果更好。为C51的算法使用了这种方法，C表示Categorical，51表示他们将值分布划分51个grid。最终在雅达利游戏 Atari Game 上取得好结果。
QR-DQN（分位数回归 Quantile Regression），使用N个分位数去描述Q值分布（这种方法比C51划分51个grid的方法更妙，我推荐看 QR-DQN - Frank Tian）
Rainbow DQN，上面提及的DQN变体很多是相互兼容的，因此 David Sliver 他们整合了这些变体，称为Rainbow。
Ape-X DQN（Distributed Prioritized Experience Replay），也是 David Sliver 他们做的。使用了Distributed training，用多个进程创建了多个actor去与环境交互，然后使用收集到的数据去训练同一个learner，用来加快训练速度。Prioritized Experience Replay（优先经验回放 PER 下面会讲）。Ape-X通过充分利用CPU资源，合理利用GPU，从而加快了训练速度。注意，这不等同于减少训练总步数。NVIDIA 有一个叫 Apex的库，用于加速计算。
Ape-X DPG（Distributed Prioritized Experience Replay），Ape-X算法将值分布RL应用到了 离散、连续动作空间的DQN 以及DPG 上。下面会讲连续动作空间的算法，Ape-X DPG 会和 D4PG 写在一起。
Recurrent Experience Replay in Distributed Reinforcement Learning ICLR 2019（自称RNN版本的Ape-X，自称当时的SotA），这是一个反面例子。请警惕 任何使用RNN 且无开源代码的DRL论文。由于hidden state 的存在，RNN会让MDPs退化成 PO-MDPs。很多论文在没有解决这个问题的情况下，宣称得到“好结果”。若有人反对我的这个看法，请提供RNN+DRL的开源代码，只有论文没用。AI论文在线质疑 - 信息门下跑狗
多种DQN变体包含在 Rainbow DQN里。一些A+B的低创工作一般不会提及。

A+B的低创工作举例（反面例子）：发表在 IEEE.2018的 V-D D3QN: the Variant of Double Deep Q-Learning Network with Dueling Architecture. - Ying Huang。D3QN可以作为DRL课程的期中作业，绝不该发论文。像 TRPO.ICML.2015的作者发表PPO的时候，也只是放在ArXiv上（PPO相比TRPO没有学术上的大创新）。Adam优化器也是在 ArXiv上。
过渡：从离散到连续动作空间的跨越（可以不看）
DQN直接训练一个Q Network 去估计每个离散动作的Q值，使用时选择Q值大的动作去执行（贪婪策略）。DDPG也训练一个 Critic Network 去估计state-action的Q值，然后把Critic Network“连在”Actor Network的后面，让Critic 为策略网络Actor 提供优化的梯度。

策略梯度的 Actor-Critic 与 对抗网络的 Generator-Discriminator很像，想要了解更多可看 曾伊言：从双层优化视角理解对抗网络GAN 。
Value-based Methods、Policy-based Methods、Policy Gradint、Actor-Critic Methods，想弄清楚它们的区别，可以看 Policy Gradient - Hado van Hasselt 2016 pdf

Policy Gradient - Hado van Hasselt 的pdf，第6页
连续的动作空间 continuous action space
一个连续动作空间例子：LunarLanderContinuous-v2 ：它与离散动作LunarLander-v2极为相似。动作矢量是两个浮点数，符号表示喷射方向，绝对值表示喷射力度。将离散改为连续动作后，月球降落器的训练时间变长，但是收敛得分变高。因此在设计环境时，遵循先易后难的原则：设计初期能用离散动作尽量别用连续动作，等后期再改成连续动作，以追求更高的得分。

另一个连续动作空间例子：双足机器人 BipdealWalker-v3，动作矢量控制足部关节的运动，尽快到达终点并消耗更少能量能得高分。下方视频中的 BipdealWalkerHardCore-v3（硬核版）加入了许多障碍，也增加了任务难度。由于障碍是随机出现的，因此训练的难度更高。

训练 4106轮（通关双足机器人硬核版）BipedalWalkerHardcore-v3_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili
​
www.bilibili.com
图标
DPG（Deterministic Policy Gradient 确定策略梯度），确定策略网络只输出一个确定的action，然后加上一个人为指定的noise 去完成探索。可以跳过DPG这篇文论，直接入门DDPG。
DDPG（Deep DPG ），可用于入门连续动作空间的DRL算法。DPG 确定策略梯度算法，直接让策略网络输出action，成功在连续动作空间任务上训练出能用的策略，但是它使用 OU-noise 这种有很多超参数的方法去探索环境，训练慢，且不稳定。
soft target update（软更新），用来稳定训练的方法，非常好用，公式是 [公式] ，其中 theta是使用梯度进行更新的网络参数，theta' 是使用了软更新的目标网络target network参数，tau略小于1。软更新让参数的更新不至于发生剧变，从而稳定了训练。从DDPG开始就广泛使用，并且在深度学习的其他领域也能看到它的身影，如 谷歌自监督 BYOL Bootstrap Your Own Latent ，看论文的公式（1），就用了soft target update
TD3（TDDD，Twin Delay DDPG），擅长调参的人才建议用，因为它影响训练的敏感超参数很多。它从Double DQN那里继承了Twin Critic，用来降低高估误差；它用来和随机策略梯度很像的方法：计算用于更新TD-error的Q值时，给action加上了噪声，用于让Critic拟合更平滑的Q值估计函数。TD3建议 延迟更新目标网络，即多更新几次网络后，再使用 soft update 将网络更新到target network上，我认为这没有多大用，后来的其他算法也不用这个技巧。TD3还建议在计算Q值时，为动作添加一个噪声，用于平滑Critic函数，在确定策略中，TD3这么用很像“随机策略”。详见 曾伊言：强化学习算法TD3论文的翻译与解读
D4PG（Distributed Distributional DDPG），这篇文章做了实验，证明了一些大家都知道好用的trick是好用的。Distributed：它像 Ape-X一样用了 多线程开了actors 加快训练速度，Distributional：它像Ape-X DPG 一样也用了 Q值分布RL（看前面的Ape-X）。DDPG探索能力差的特点，它也完好无缺地继承了。
在C51算法论文标题中，Distributional Perspective 指 Q值分布的表示。在Ape-X DQN 标题中， Distributed training 指分布式训练。我曾混淆过它们。
====确定策略梯度↑ ↓ 随机策略梯度====

Stochastic Policy Gradient 随机策略梯度，随机策略的探索能力更好。随机策略网络会输出action的分布（通常输出高斯分布 均值 与 方差，少数任务下用其他分布），探索的噪声大小由智能体自己决定，更加灵活。但是这对算法提出了更高的要求。
A3C（Asynchronous Advantage Actor-Critic），Asynchronous 指开启多个actor 在环境中探索，并异步更新。原本DDPG的Critic 是 Q(s, a)，根据state-action pair 估计Q值，优势函数只使用 state 去估计Q值，这是很好的创新：降低了随机策略梯度算法估计Q值的难度。然而优势函数有明显缺陷：不是任何时刻 action 都会影响 state的转移（详见 Dueling DQN），因此这个算法只适合入门学习「优势函数 advantage function」。如果你看到新论文还在使用A3C，那么你要怀疑其作者RL的水平。此外，A3C算法有离散动作版本，也有连续动作版本。A2C 指的是没有Asynchronous 的版本。
TRPO（Trust Region Policy Optimization），信任域 Trust Region。连续动作空间无法每一个动作都搜索一遍，因此大部分情况下只能靠猜。如果要猜，就只能在信任域内部去猜。TRPO将每一次对策略的更新都限制了信任域内，从而极大地增强了训练的稳定性。可惜信任域的计算量太大了，因此其作者推出了PPO，如果你PPO论文看不懂，那么建议你先看TRPO。如果你看到新论文还在使用TRPO，那么你要怀疑其作者RL的水平。
PPO（Proximal PO 近端策略搜索），训练稳定，调参简单，robust（稳健、耐操）。PPO对TRPO的信任域计算过程进行简化，论文中用的词是 surrogate objective。PPO动作的噪声方差是一个可训练的矢量（与动作矢量相同形状），而不由网络输出，这样做增强了PPO的稳健性 robustness。
PPO+GAE（Generalized Advantage Estimation），训练最稳定，调参最简单，适合高维状态 High-dimensional state，但是环境不能有太多随机因数。GAE会根据经验轨迹 trajectory 生成优势函数估计值，然后让Critic去拟合这个值。在这样的调整下，在随机因素小的环境中，不需要太多 trajectory 即可描述当前的策略。尽管GAE可以用于多种RL算法，但是她与PPO这种On-policy 的相性最好。
PPG（Proximal Policy Gradient），A3C、PPO 都是同策略 On-policy，它要求：在环境中探索并产生训练数据的策略 与 被更新的策略网络 一定得是同一个策略。她们需要删掉已旧策略的数据，然后使用新策略在环境中重新收集。为了让PPO也能用 off-policy 的数据来训练，PPG诞生了，思路挺简单的，原本的On-policy PPO部分该干啥干啥，额外引入一个使用off-policy数据进行训练的Critic，让它与PPO的Critic共享参数，也就是Auxiliary Task，参见 Flood Sung：深度解读：Policy Gradient，PPO及PPG ，以及白辰甲：强化学习中自适应的辅助任务加权(Adaptive Auxiliary Task Weighting)。这种算法并不是在任何情况下都能比PPO好，因为PPG涉及到Auxiliary task，这要求她尽可能收集更多的训练数据，并在大batch size 下面才能表现得更好。
Interpolated Policy Gradient NIPS.2017，反面例子，它试图基于 On-policy TRPO 改出一个 能利用 off-policy 数据的TRPO来（就像PPO→PPG），然而Interpolated Policy Gradient 强行地、错误地使用off-policy 数据对Critic 进行训练。结果在其论文中放出的结果中，它的性能甚至比A3C还差，只是比TRPO、DDPG略好（但是它故意没有和比它好的算法在同一个任务下比较，论文结果很诚实，但是用事实说谎）。
Soft Q-learning（Deep Energy Based Policy）是SAC的前身，最大熵算法的萌芽，她的作者后来写出了SAC（都叫soft ***），你可以跳过Soft QL，直接看SAC的论文。黄伟：Soft Q-Learning论文阅读笔记
SAC（Soft Actor-Critic with maximum entropy 最大熵），训练很快，探索能力好，但是很依赖Reward Function，不像PPO那样随便整一个Reward function 也能训练。PPO算法会计算新旧策略的差异（计算两个分布之间的距离），并让这个差异保持在信任域内，且不至于太小。SAC算法不是on-policy算法，不容易计算新旧策略的差异，所以它在优化时最大化策略的熵（动作的方差越大，策略的熵越高）。
SAC（Automating Entropy Adjustment/ Automating Temperature Parameter [公式] 自动调整温度系数并维持策略的熵在某个值附近）一般我们使用的SAC是这个版本的SAC，它能自动调整一个叫温度系数alpha 的超参数（温度越高，熵越大）。SAC的策略网络的优化目标=累计收益+ alpha*策略的熵。一般在训练后期，策略找到合适的action分布均值时，它的action分布方差越小，其收益越高，因而对“累计收益”进行优化，会让策略熵倾向于减小。SAC会自动选择合适的温度系数，让策略的熵保持一种适合训练的动态平衡。SAC会事先确定一个目标熵 target entropy（论文作者的推荐值是 log(action_dim)），如果策略熵大于此值，则将alpha调小，反之亦然。从这个角度看，SAC就不是最大化策略熵了，而是将策略熵限制在某个合适大小内，这点又与PPO的“保持在信任域内，且不至于太小”不谋而合。
注意，SAC+加权重要性采样，这类A+B的工作不会在这里提及。一些泛用性强的奖励塑形（Reward Shaping）方案会以后补充。

环境随机因素较少是什么意思？相比 双足机器人 BipdealWalker-v3 与 硬核版 BipdealWalkerHardCore-v3，后者的环境随机因素很多。因为道路前方会随机出现难以跨越的障碍（看上面的视频），我们人类拥有常识，清楚障碍出现与动作完全无关（独立事件），可智能体她事先不知道呀！如PPO+GAE 使用 轨迹trajectory 进行更新，如果每一次在线采样得到的轨迹差别太大，且不能体现障碍的出现与动作无关，那么智能体就会乱学，只能增加每轮与环境的交互次数来获得更多训练数据，导致训练极慢。2018年前的DRL算法无法通关，能通关的代码在此，自己去试「强化学习小雅：ElegantRL」

====下面的内容可能要移动到别的文章里====

基于SAC的这些特性（优化目标=累计收益+ alpha*策略的熵），SAC对Reward Function 比较敏感，它要求让reward 乘以一个数值 reward scaling 调整其大小，调整gamma等会影响Q值大小的操作都会对SAC造成影响。（而PPO因为能对 Q值做正则化等原因，而不像SAC一样对Q值变化这么敏感）。

既然评论区有人问 
@张华卿
，我就先在这里写：SAC相比PPO、TD3，为何不适合最优策略有大量边界动作的任务？什么叫有大量边界动作的任务？比如控制机器人移动，在不限制能耗的情况下，全速移动经常是最优策略。产生连续空间的探索动作有以下3类方式：

TD3：动作→tanh→添加噪声（方差由人类指定）→clip
PPO：动作→添加噪声（所有state共用同个噪声方差）→tanh
SAC：动作→添加噪声（不同state对应不同噪声方差）→tanh
TD3添加噪声在tanh与clip之间，因此容易在探索中产生大量边界动作。PPO和SAC在tanh之前，所以不容易产生。PPO相当于将TD3中人为指定的动作噪声方差，换成了一个可训练的参数，不同的state下都使用同一个噪声方差，因此这个方差不会变得很离谱。SAC的噪声方差会由网络网络根据不同的state输出对应的数值，某个状态下的方差可能会很大。

在此基础上，PPO计算新旧策略的熵使用的是经过tanh之前的动作，而SAC计算策略的熵使用了tanh之后的动作，因而在动作均值接近动作边界时，即便方差很大，它策略的熵不见得很大。因此SAC为了抵消tanh的影响，在计算策略熵的时候，添加了 tanh(a) 的导数项 [公式] 作为修正（2021年前，知乎上其他写这一块的，我没见过弄懂这个的），加上极小值 epsilon 是为了防止log计算溢出。然而，当动作接近边界值 -1 或 1时，这里的计算误差非常大，甚至会导致梯度方向出现错误。

如下，画出SAC算法的log_prob，x轴是动作方差的log，y轴是策略的熵。m是动作均值，n是随机产生的噪声数值（随机产生了一个0.6的噪声）。红色曲线是理论理想情况，可以看到，最大化策略的熵，会让动作方差往曲线最高点的方向优化（鼠标位置）。然而，蓝色曲线是实际计算中不得不采用的方案 epslion=0.00001，可见蓝色曲线在实际动作（m+dn）靠近较大值的情况下往上翘。当实际动作比较大时（体现为tanh(action)接近动作边界 -1 或 +1），动作分布的方差 d 可能会朝错误方向优化。（选择更小的 epslion等很容易想到的方法，无济于事）


使用谷歌浏览器的插件 desmos绘制，鼠标指向的是红色曲线的极值点
m+dn 表示 action mean + action std * Gassian Noise。无论noise是正数或者负数，结果都一样。这里的部分算法相对复杂，以后这里的内容会移动到新的文章去。见姊妹篇「曾伊言：深度强化学习调参技巧：以D3QN、TD3、PPO、SAC算法为例」

混合的动作空间 hybrid action space

王者荣耀 - 腾讯绝悟 论文截图，既有离散动作，也有连续动作

在实际任务中，混合动作的需求经常出现：如王者荣耀游戏既需要离散动作（选择技能），又需要连续动作（移动角色）。只要入门了强化学习，就很容易独立地想出以下这些方法，所以我没有把它们放在前面：

强行使用DQN类算法，把连续动作分成多个离散动作：不建议这么做，这破坏了连续动作的优势。一个良性的神经网络会是一个平滑的函数（k-Lipschitz 连续），相近的输入会有相似的输出。在连续的动作空间开区间[-1, +1]中，智能体会在学了-1，+1两个样本后，猜测0的样本可能介于 -1，+1 之间。而强行将拆分为离散动作 -1，0，+1之后（无论拆分多么精细），它都猜不出 0的样本，一定要收集到 0的样本才能学习。此外，精细的拆分会增加离散动作个数，尽管更加逼近连续动作，但会增加训练成本。
SAC for Discrete Action Space，把输出的连续动作当成是离散动作的执行概率：SAC for Discrete Action Space 这个算法提供了将连续动作算法SAC应用在离散动作的一条技术路线：把这个输出的动作矢量当成每个动作的执行概率。一般可以直接把离散动作部分全部改成连续动作，然后套用连续动作算法，这方法简单，但是不一定最好的。
P-DQN （Parameterized DQN），把DQN和DDPG合起来：Q network 会输出每个动作对应的Q值，执行的时候选择Q值高的动作。DDPG与其他策略梯度算法，让Critic预测 state-action的Q值，然后用Critic 提供的梯度去优化Actor，让Actor输出Q值高的动作。现在，对于一个混合动作来说，我们可以让Critic学习Q Network，让Critic也为每个离散动作输出对应的Q值，然后用Critic中 arg max Qi 提供梯度优化Actor。这是很容易独立想出来的方法，相比前两个方案缺陷更小。
H-PPO （Hybrid PPO），同时让策略网络输出混合动作。连续动作（策略梯度）算法中：DDPG、TD3、SAC使用 状态-动作值函数 Q(state, action)，A3C、PPO使用 状态值函数 Q(state)。离散动作无法像连续动作一样将一个action输入到 Q(state, action) 里，因此 Hybird PPO选择了PPO。于是它的策略网络会像Q Network 一样为离散动作输出不同的Q值，也像PPO 一样输出连续动作。
详见Hybird-PPO 2019-03 这篇文章的 Related Work。知乎上也有 黑猫紧张：PN-46: H-PPO for Hybrid Action Space (IJCAI 2019)

改进经验回放，以适应稀疏奖励 sparse reward
训练LunarLander安全降落，它的奖励reward 在降落后+200，坠毁-100。当它还在空中时做任何动作都不会得到绝对值这么大的奖励。这样的奖励是稀疏的。一些算法（（其实它的奖励函数会根据飞行器在空中的稳定程度、燃料消耗给出一个很小的reward，在这种）

Prioritized sweeping 优先清理：根据紧要程度调整样本的更新顺序，优先使用某些样本进行更新，用于加速训练，PER就是沿着这种思想发展出来的
PER（优先经验回放 Prioritized Experience Replay）使用不同顺序的样本进行对网络进行训练，并将不同顺序对应的Q值差异保存下来，以此为依据调整样本更新顺序，用于加速训练。
HER（后见经验回放 Hindsight Experience Replay）构建可以把失败经验也利用起来的经验池，提高稀疏奖励下对各种失败探索经验的利用效率。
这种操作需要消耗CPU算力去完成。在奖励不稀疏的环境下，用了不会明显提升。

多智能体算法 MultiAgent RL
我写不了，群里面有人写了，等他写完后我会直接放上他的链接。这里有部分多智能体算法的代码以及少量介绍：starry-sky6688/StarCraft 在星际争霸环境 复现了多种多智能体强化学习算法

若智能体间通信没有受到限制（不限量，无延迟），那么我们完全可以把多智能体当成单智能体来处理。适用于部分可观测的MDPs的算法（Partially observable MDPs），在多智能体任务中，每个视角有限的智能体观察到的只是 partially observable state。很多多智能体算法会参与 PO-MDPs 的讨论。

分层强化学习 Hierarchical RL
神经网络有一个缺陷（特性）：在数据集A上面训练的网络，拿到数据集B上训练后，这个网络会把数据集A学到的东西忘掉（灾难性遗忘 Catastrophic forgetting 1999）。如果我让智能体学游泳，再让它学跑步，它容易把游泳给忘了（人好像也会这样，不够没有它们那么严重）。深度学习领域有「迁移学习」、强化学习领域有「分层强化学习」在试图解决这些难题。

FuNs，分级网络 FeUdal Networks ，分层强化学习不再用单一策略去解决这些更复杂的问题，而是将策略分为上层策略与多个下层策略 sub-policy 。上层策略会根据不同的状态决定使用哪个下层策略。它使用了同策路on-policy的A3C算法
HIRO，使用异策略进行校正的分层强化学习 HIerarchical Reinforcement learning with Off-policy correction，警惕HIRO这个算法：FuN使用同策路on-policy的A3C算法，HIRO使用异策略off-policy的TD3算法，这个让我警惕：我个人认为不能像HIRO那样去使用TD3算法。
Option-Critic，有控制权的下层策略，让将上层的策略和下层策略的控制权也当成是可以学习的，让下层的策略学习把“决定使用哪个策略的选择权”交还给上层策略的时机，这是一种隐式的分层强化学习方案，我没有复现过这个算法，我不确定这是否真的有效。
我不懂逆向强化学习，请看别人写的 张楚珩：【强化学习算法 18】FuN ，张楚珩：【强化学习算法 19】HIRO ，张楚珩：【强化学习算法 20】Option-Critic

逆向强化学习 Inverse RL 与 模仿学习 Imitation Learning
强化学习会在回报函数 Reward function的指导下探索训练环境，并使用未来的期望收益来强化当前动作，试图求出更优的策略。然而，现实中不容易找到需要既懂任务又懂RL的人类去手动设计Reward function。

以 LunarLander为例子：降落+200 坠毁-100，消耗燃料会扣0~100。其实只有这些我们也能用很长的时间训练得到能安全降落的飞行器。但实际上，我们还可以根据飞行器的平稳程度给它每步一位数的奖惩，根据飞行器距离降落点的距离给他额外的奖励。这些很细节的调整可以减少智能体的训练时间。所以我前面建议：如果训练环境 Reward function 都是初学者写的，那就用PPO。等到 Reward function 设计得更合理之后，才适合用SAC。
强化学习：训练环境+DRL算法+Reward Function = 搜索出好的策略
逆向强化学习：训练环境+IRL算法+好的策略 = 逆向得到Reward Function
逆向强化学习为了解决这个问题，提出：通过模仿好的策略 去反向得到 Reward function。我不懂反向强化学习，如果强行写解释也只能翻译其他人的综述：A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress

基于模型的强化学习算法 Model-based RL（重点介绍MuZero）
这里的「模型」指：状态转移模型。离散状态空间下的状态转移模型可以用 状态转移矩阵去描述。基于模型的算法需要将状态转移模型探索出来（或由人类提供），而 无模型算法 model-free RL 不需要探索出模型，它仅依靠智能体在环境中探索 rollout 得到的一条条 trajectory 中记录的 environment transition (s, a, r, next state) 即可对策略进行更新。

我对无模型算法不够了解，如果强行写解释也只能翻译其他人的综述：Model-based Reinforcement Learning: A Survey 。OpenAI 提供了一些简单的代码： SpinningUp Model-based RL 。

近年来受到最多圈外人关注的 model-based RL 是 MuZero。在下棋、雅达利游戏这种状态转移模型相对容易拟合的离散动作空间任务中，MuZero取得了非常不错的表现。它有三个网络：

编码器：输入连续观测到的几个state，将其编码成 latent state。为何非要使用 latent state 而不直接使用 state？ 在当前state 下做出action 后，并不会转移到某个确切的状态，next state 是一个不容易直接描述的分布。因此接下来的生成器不会（也无法）直接预测 next state，只能预测 latent state。
预测器：输入当前观测到的state，生成执行每个动作的概率，并预测执行每个动作的value （Q值，我不反对将它粗略地理解为DQN的 Q Network）。
生成器：输入当前观测到的state，生成 执行每个离散动作后会转移到的 latent state 以及对应的 Reward（这是单步的Reward，不是累加得到的Q值）。生成器就是MuZero 这个model-based RL算法 学到的状态转移模型。
如果离散动作的数量很多（如围棋），那么MuZero 会使用MCTS（Monte Carlo tree search 蒙特卡洛树搜索），剪除低概率的分支并估计Q值（论文里用 [公式] ），具体剪去多少分支要看有多少算力。

model-based RL 学到状态转移模型之后，就能在探索环境之前想象出接下来几步的变化，然后基于环境模型做规划，减少与环境的交互次数。（在model-based RL 中经常可以读到 Imagination，planning，dream这些词）。这里顺便解释一下 MuZero 的 Mu 是什么意思？ 解释来自MuZero Intuition - Julian Schrittwieser - What's in a name?

希腊字母 μ，用来表示强化学习算法学到的策略模型
夢（梦）。MuZero使用预测器预测智能体在环境中的下一步。
無（无）。MuZero（AlphaZero）不需要像前身AlphaGo（AlphaMaster）那依赖人类知识去学习。
推荐看知乎问题：如何评价DeepMind新提出的MuZero算法？ 下面的 什么名字可以吸粉的回答（他的翻译：representation function 编码器，prediction function 预测器，dynamics function生成器 比较直译好） 与 Evensgn的回答 （Value Prediction Network 值得看一看）。

好用的强化学习算法是？
没有很多需要调整的超参数。D3QN、SAC超参数较少，且SAC可自行调整超参数 [公式]
超参数很容易调整或确定。SAC的 reward scaling 可以在训练前直接推算出来。PPO超参数的细微改变不会极大地影响训练
训练快，收敛稳、得分高。看下面的 学习曲线 learning curve

弯弯曲曲的学习曲线很正常，图片截取自 Ape-X 与 SAC 论文
学习曲线怎么看？
横轴可以是训练所需的步数（智能体与环境交互的次数）、训练轮数（达到固定步数、失败、通关 就终止终止这一轮的训练episode）、训练耗时（这个指标还与设备性能有关）
纵轴可以是 每轮得分（ 每一轮的每一步的reward 加起来，episode return），对于没有终止状态的任务，可以计算某个时间窗口内reward之和
有时候还有用 plt.fill_between 之类的上下std画出来的波动范围，用于让崎岖的曲线更好看一点。先选择某一段数据，然后计算它的均值，再把它的标准差画出来，甚至可以画出它的上下偏差（琴形图）。如果同一个策略在环境随机重置后得分相差很大，那么就需要多测几次。
好的算法的学习曲线应该是？
训练快，曲线越快达到某个目标分数 target reward （需要多测几次的结果才有说服力）
收敛稳，曲线后期不抖动（曲线在前期剧烈抖动是可以接受的）
得分高，曲线的最高点可以达到很高（即便曲线后期下降地很厉害也没关系，因为我们可以保存整个训练期间“平均得分”最高的模型）
一般地，学习曲线长这个样子↙，过于平滑的学习曲线↘应该激起怀疑：


对比诚实的DQN训练结果，左图为Nature 2015 DQN的诚实结果。右图为黄鸿基的不诚实结果。
上图右侧粉色曲线必定是造假的结果，而且极为拙劣：造假者因为没有入门强化学习（甚至没入门机器学习）而画出来这样的曲线。审稿人也没有入门强化学习。若想了解更多请移步那个知乎问题，不要在这里讨论：

知乎问题：如何看待南京邮电大学学生（黄鸿基）2020申请到加州理工EE系唯一一个大陆PhD？ （加州理工是好学校）
搜索引擎“南京邮电大学 黄鸿基 冒充北大 加州理工 光滑的曲线”（南京邮电是好学校）
其造假论文在此 Deep Reinforcement Learning for UAV Navigation Through Massive MIMO Technique ，投给了 IEEE Transactions on Vehicular Technology （IEEE是好期刊）
叶强：《强化学习》第三讲 动态规划寻找最优策略 ：“本讲着重讲解了利用动态规划来进行强化学习，具体是进行强化学习中的“规划”，也就是在已知模型的基础上判断一个策略的价值函数，并在此基础上寻找到最优的策略和最优价值函数，或者直接寻找最优策略和最优价值函数。本讲是整个强化学习课程核心内容的引子。”

很多刚入门的会理解错加粗的话，换我来解释：强化学习会以“寻找最优策略”为目标，试图找出最优策略。「试图」这个词很重要，实际使用中，增加训练量不一定保证学习曲线会一直上升，甚至不能保证上升趋势。上面黄鸿基由于缺乏对DRL的理解，导致它伪造的曲线后期是平的（这个人没理解什么叫收敛）。下图截取自Parametrized DQN，可以看到，在一个不简单的任务中，背景浅色的曲线非常波折，得平滑后才能得到有较好可视化效果的曲线。


截图来自 Parametrized DQN 的图5，这是正常的learning curve
交流：深度学习ElegantRL QQ群号 1163106809
等群里面的熟悉多智能体的人写完，我会更新他们的多智能体总结的链接

这里提及的群叫 深度学习ElegantRL QQ群号 1163106809。严格禁止广告、主要聊深度强化学习的QQ群在此（聊深度学习、机器学习亦可。因为群里面有学生，因此聊金融强化学习时，只要不聊自己的盈亏情况即可），进群不回答强化学习问题者将被视为广告。入群后自己的群称呼称建议改为“地区-昵称-研究方向/应用方向”


交流：评论区的好问题会在这里回答，或者直接写入正文。
