---
title: A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning
---

## Designing agents to act near-optimally in stochastic sequential domains is a challenging problem that has been studied in a variety of settings.
## When the domain is known, analytical techniques such as dynamic programming (DP) [Bellman, 1957] are often used to find optimal policies for the agent.
## When the domain is initially unknown, reinforcement learning (RL) [Sutton and Barto, 1998] is a popular technique for training agents to act optimally based on their experiences in the world. However, in much of the literature on these topics, small-scale environments were used to verify solutions. For example the famous taxi problem has only 500 states [Dietterich, 2000]. This contrasts with recent success stories in domains previously considered unassailable, such as 9×9 Go [Silver et al., 2012], a game with approximately 1038 states. An important factor in creating solutions for such large-scale problems is the use of linear function approximation [Sutton, 1996, Silver et al., 2012, Geramifard et al., 2011]. This approximation technique allows the long-term utility (value) of policies to be represented in a low-dimensional form, dramatically decreasing the number of parameters that need to be learned or stored. This tutorial provides practical guidance for researchers seeking to extend DP and RL techniques to larger domains through linear value function approximation. We introduce DP and RL techniques in a unified frame-work and conduct experiments in domains with sizes up to ∼ 150 million state-action pairs.
