---
title: 周博磊
---

## 作者：周博磊
链接：https://www.zhihu.com/question/316626294/answer/627373838
## 请问DeepMind和OpenAI身后的两大RL流派有什么具体的区别？ - 周博磊的回答 - 知乎
https://www.zhihu.com/question/316626294/answer/627373838
## 谢邀。这是个很好的问题，能看出这个差别说明RL还是上路了。这学期我正好在教Reinforcement learning的课程（课程主页Reinforcement Learning）。第一次开课，花费挺多时间备课以及储备RL的前沿进展，ICCV之后也许可以更详细地总结一下。总的来说，确实DM和OpenAI两家的工作有明显的派别差异，对RL的理解非常不同，忠于Value-based RL和Policy-based RL的差别。这跟其中的两家主脑人物的RL学派直接关联。David Silver毫无疑问是DM的主脑人物之一。David的博士导师是Richard Sutton，所以是非常受Sutton的RL价值观影响。Sutton推崇的是正统的Value-based RL。Sutton就是那本畅销书 Reinforcement learning: an introduction的作者之一。读过这本书的同学应该可以发现，这本书是按照经典方法论来组织，比如说Markov Decision Process, Dynamic Programming, Monte Carlo Methods, Temporal Difference Learning等，这些都是传统做优化和控制论里的东西。30年前的RL确实就是control theory背景的人在做，还没做机器学习这帮小屁孩什么事儿。这里还有个有意思的点是，这本500多页的RL畅销教材，关于现在大行其道的Policy-based RL (Policy gradient)的相关方法，只有不到20页的内容，可见Sutton对Policy-based RL的态度。所以David最早在DeepMind里面也是推行Value-based RL的思想，搞出了Deep Q-learning, DDPG之类的东西。至于说后来的A3C，已经是不得不屈从于Policy-based RL的高效率。另外一方面，OpenAI背后的派别是Berkeley帮，主要工作是围绕Pieter Abbeel以及他的两位superstar博士生Sergey Levine和John Schulman。Levine和Schulman可以说在现今的RL圈子里如日中天，两人都做出了非常有影响力的工作。Levine把Guided Policy Search (GPS)用到robotics里面，使得小样本RL也能学习。Schulman理论功底扎实，为人低调，他的TRPO以及后来的PPO，都是RL必用算法（这里有能看懂TRPO论文推导的同学私信我:)，我请你过来visit，我敬你是条汉子）。Berkeley帮的明显特征是极度推崇Policy-based RL。用过RL的同学应该知道，policy-based RL以及衍生出的model-based RL比value-based RL效率高一个量级，这跟Abbeel和Sergey的机器人背景关系非常大。在机器人的应用中，sample-efficiency非常重要。不像DeepMind随便就可以跑million级别数量的游戏仿真，机械手臂这玩意其实是非常容易坏的，而且价格不菲，在构建RL算法的时候不得不从sample efficiency角度出发，所以Levine提出了GPS以及相关的一堆东西如imitation learning，inverse RL, model-based RL，包括他的门徒Chelsea Finn做的关于meta-learning之类的东西，都是从这一点出发，跟他的robotics背景也相符。所以这两个派别差异确实还是挺大。另外，这跟两家公司的定位也有关系，比如说DeepMind着眼于Go和Starcraft这样的AI明珠问题，可能确实Value-based RL+search的办法更work。OpenAI强调Open，大众普及RL，着眼于一些机器人应用和相对小规模的RL问题，Policy-based RL以其优秀的效率和稳定性更胜一筹。可惜OpenAI里面的人已经走得差不多了，创立时候定义自己是non-profit organization，理想很丰满现实很残酷，啧啧啧。一句话，黑喵白喵抓着老鼠就是好喵。以PPO为核心的Policy-based RL方法目前处于绝对领先位置，有着广泛的群众基础。DeepMind着眼的那些AI明珠问题不是我等群众老百姓可以企及的，坐等吃瓜就好。
