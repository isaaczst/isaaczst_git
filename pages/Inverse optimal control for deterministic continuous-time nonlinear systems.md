- pdf http://publish.illinois.edu/science-of-security-lablet/files/2017/03/Inverse-Optimal-Control-for-Deterministic-Continuous-time-Nonlinear-Systems.pdf
# Abstract:
	- Inverse optimal control is the problem of computing a cost function with respect to which observed state and input trajectories are optimal.
	- We present a new method of inverse optimal control based on minimizing the extent to which observed trajectories violate first-order necessary conditions for optimality.
	- We consider continuous-time deterministic optimal control systems with ^^a cost function that is a linear combination of known basis functions^^.
	- We compare our approach with three prior methods of inverse optimal control.
	- We demonstrate the performance of these methods by performing simulation experiments using a collection of nominal system models.
	- We compare the robustness of these methods by analysing how they perform under perturbations to the system.
	- To this purpose, we consider two scenarios: one in which we exactly know the set of basis functions in the cost function, and another in which the true cost function contains an unknown perturbation.
	- Results from simulation experiments show that our new method is more computationally efficient than prior methods, performs similarly to prior approaches under large perturbations to the system, and better learns the true cost function under small perturbations.
# I. INTRODUCTION
- In the problem of optimal control we are asked to find input and state trajectories that minimize a given cost function. 
  collapsed:: true
	- In the problem of inverse optimal control we are asked to find a cost function with respect to which observed input and state trajectories are optimal.
	- Methods of inverse optimal control are beginning to find widespread application in robotics.
	- In this paper, we consider this problem under deterministic continuous-time nonlinear systems and cost functions modeled by a linear combination of known basis functions.
	- Three existing methods which solve this problem are the following:
- The max-margin inverse reinforcement learning method of Abbeel and Ng [1].
	- This method is motivated by the problem of efficiently automating vehicle navigation tasks which currently require human expert operation.
	- This method works by trying to learn a cost function that, when minimized, yields a trajectory with similar features as the expert.
	- This method recently contributed to a framework which enables autonomous helicopter aerobatic flight based on observations of human expert pilots.
- The maximum-margin planning method of Ratliff, et al.[2].
	- This method shares the motivation of Abbeel and Ng, and works by minimizing a regularized risk function using an incremental subgradient method.
	- This method contributed to a framework which mimics human driving of an autonomous mobile robot in complex off-road terrain
- The method of Mombaur, et al. which we will call bilevel inverse optimal control [3].
	- This work is motivated by the problem of generating humanoid robot behavior which is similar to natural human motion.
	- This method works by minimizing the sum squared error between
	  predicted and observed trajectories.
	- This method is applied to develop a model of human goal-oriented
	  locomotion in the plane (i.e. paths taken during goaloriented walking tasks) using observations from motion capture, and implement the model on a humanoid robot.
- Despite differences in how learning is performed, these methods share common structure.
	- One goal of this paper is to explain this common structure and compare these methods on a set of example problems.
	- One common component to these algorithms is particularly important.
	- Each method contains an inner loop which computes a predicted trajectory by minimizing a candidate cost function.
	- In other words, each method solves a forward optimal control problem repeatedly in an inner loop.
	- This can often lead to a computational bottleneck.
	- The other goal of this paper is to develop an approach which does not solve a forward optimal control problem repeatedly in an inner loop.
	- ^^Our method, inspired by ideas from inverse optimization in [4], makes the assumption that observations may arise from a system which is only approximately optimal.^^
	- We define how optimal a trajectory is based on how closely it satisfies necessary conditions for optimal control.
	- This assumption allows us to define residual functions based on these necessary conditions which, when minimized over the unknown parameters, yields a solution which makes the observations most optimal.
	- As we will show, this new approach reduces to solving a matrix Riccati differential equation followed by one least-squares minimization.
	- CN
	  - å°½ç®¡å­¦ä¹ çš„æ–¹å¼æœ‰æ‰€ä¸åŒï¼Œä½†è¿™äº›æ–¹æ³•å…·æœ‰å…±åŒçš„ç»“æ„ã€‚ 
	  - æœ¬æ–‡çš„ä¸€ä¸ªç›®æ ‡æ˜¯è§£é‡Šè¿™ç§é€šç”¨ç»“æ„ï¼Œå¹¶åœ¨ä¸€ç»„ç¤ºä¾‹é—®é¢˜ä¸Šæ¯”è¾ƒè¿™äº›æ–¹æ³•ã€‚ è¿™äº›ç®—æ³•çš„ä¸€ä¸ªå…±åŒç»„æˆéƒ¨åˆ†å°¤ä¸ºé‡è¦ã€‚ æ¯ä¸ªæ–¹æ³•éƒ½åŒ…å«ä¸€ä¸ªå†…éƒ¨å¾ªç¯ï¼Œå®ƒé€šè¿‡æœ€å°åŒ–å€™é€‰æˆæœ¬å‡½æ•°æ¥è®¡ç®—é¢„æµ‹è½¨è¿¹ã€‚ æ¢å¥è¯è¯´ï¼Œæ¯ç§æ–¹æ³•åœ¨å†…å¾ªç¯ä¸­é‡å¤è§£å†³å‰å‘æœ€ä¼˜æ§åˆ¶é—®é¢˜ã€‚ è¿™é€šå¸¸ä¼šå¯¼è‡´è®¡ç®—ç“¶é¢ˆã€‚ æœ¬æ–‡çš„å¦ä¸€ä¸ªç›®æ ‡æ˜¯å¼€å‘ä¸€ç§ä¸ä¼šåœ¨å†…ç¯ä¸­é‡å¤è§£å†³å‰å‘æœ€ä¼˜æ§åˆ¶é—®é¢˜çš„æ–¹æ³•ã€‚ æˆ‘ä»¬çš„æ–¹æ³•å—åˆ° [4] ä¸­é€†ä¼˜åŒ–æ€æƒ³çš„å¯å‘ï¼Œå‡è®¾è§‚æµ‹å€¼å¯èƒ½æ¥è‡ªä»…è¿‘ä¼¼æœ€ä¼˜çš„ç³»ç»Ÿã€‚ æˆ‘ä»¬æ ¹æ®è½¨è¿¹æ»¡è¶³æœ€ä¼˜æ§åˆ¶çš„å¿…è¦æ¡ä»¶çš„ç¨‹åº¦æ¥å®šä¹‰è½¨è¿¹çš„æœ€ä¼˜ç¨‹åº¦ã€‚ è¿™ä¸ªå‡è®¾å…è®¸æˆ‘ä»¬æ ¹æ®è¿™äº›å¿…è¦æ¡ä»¶å®šä¹‰æ®‹å·®å‡½æ•°ï¼Œå½“åœ¨æœªçŸ¥å‚æ•°ä¸Šæœ€å°åŒ–æ—¶ï¼Œäº§ç”Ÿä¸€ä¸ªä½¿è§‚æµ‹å€¼æœ€ä½³çš„è§£å†³æ–¹æ¡ˆã€‚ æ­£å¦‚æˆ‘ä»¬å°†å±•ç¤ºçš„ï¼Œè¿™ç§æ–°æ–¹æ³•ç®€åŒ–ä¸ºæ±‚è§£çŸ©é˜µ Riccati å¾®åˆ†æ–¹ç¨‹ï¼Œç„¶åè¿›è¡Œæœ€å°äºŒä¹˜æœ€å°åŒ–ã€‚
- It is unclear at this point how all of these methods compare in terms of prediction accuracy, computational complexity and robustness to system perturbations.
	- In this paper, we explore the performance of these methods using three example systems:
	- (1) linear quadratic regulation,
	- (2) quadratic regulation of a kinematic unicycle, and
	- (3) calibration of an elastic rod.
	- We compare the robustness of these methods by analysing how they perform under perturbations to the system.
	- To this purpose, we consider two scenarios: one in which we exactly know the set of basis functions in the cost function, and another in which the true cost function contains an unknown perturbation.
	- Results from simulation experiments show that our new method is more computationally efficient than prior methods, performs similarly to prior approaches under large perturbations to the system, and better learns the true cost function under small perturbations
	- CN
	  - ç›®å‰å°šä¸æ¸…æ¥šæ‰€æœ‰è¿™äº›æ–¹æ³•åœ¨é¢„æµ‹ç²¾åº¦ã€è®¡ç®—å¤æ‚æ€§å’Œå¯¹ç³»ç»Ÿæ‰°åŠ¨çš„é²æ£’æ€§æ–¹é¢çš„æ¯”è¾ƒã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªç¤ºä¾‹ç³»ç»Ÿæ¢ç´¢è¿™äº›æ–¹æ³•çš„æ€§èƒ½ï¼šï¼ˆ1ï¼‰çº¿æ€§äºŒæ¬¡è°ƒèŠ‚ï¼Œï¼ˆ2ï¼‰è¿åŠ¨å­¦ç‹¬è½®è½¦çš„äºŒæ¬¡è°ƒèŠ‚ï¼Œä»¥åŠï¼ˆ3ï¼‰å¼¹æ€§æ†çš„æ ¡å‡†ã€‚ æˆ‘ä»¬é€šè¿‡åˆ†æå®ƒä»¬åœ¨ç³»ç»Ÿæ‰°åŠ¨ä¸‹çš„è¡¨ç°æ¥æ¯”è¾ƒè¿™äº›æ–¹æ³•çš„ç¨³å¥æ€§ã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸¤ç§æƒ…å†µï¼šä¸€ç§æ˜¯æˆ‘ä»¬ç¡®åˆ‡çŸ¥é“æˆæœ¬å‡½æ•°ä¸­çš„åŸºå‡½æ•°é›†ï¼Œå¦ä¸€ç§æ˜¯çœŸå®æˆæœ¬å‡½æ•°åŒ…å«æœªçŸ¥æ‰°åŠ¨ã€‚ ä»¿çœŸå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–°æ–¹æ³•æ¯”ç°æœ‰æ–¹æ³•è®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œåœ¨å¯¹ç³»ç»Ÿçš„å¤§æ‰°åŠ¨ä¸‹ä¸ç°æœ‰æ–¹æ³•ç›¸ä¼¼ï¼Œå¹¶ä¸”åœ¨å°æ‰°åŠ¨ä¸‹æ›´å¥½åœ°å­¦ä¹ çœŸå®æˆæœ¬å‡½æ•°
- The rest of the paper proceeds as follows.
	- In Section II we discuss related work and note the variety of problems to which inverse optimal control and related methods are applied.
	- In Section III we describe the class of systems we consider, and the associated inverse optimal control problem.
	- In Section IV we describe the existing methods of inverse optimal control with which we compare our new method [1]â€“[3].
	- In Section V we develop our new method based on necessary conditions for optimal control.
	- In Section VI we describe simulation experiments,and Section VII presents results and discussion.
	- e
	  - æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†å¦‚ä¸‹ã€‚ åœ¨ç¬¬äºŒèŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ç›¸å…³å·¥ä½œï¼Œå¹¶æ³¨æ„åˆ°é€†ä¼˜åŒ–æ§åˆ¶å’Œç›¸å…³æ–¹æ³•æ‰€åº”ç”¨çš„å„ç§é—®é¢˜ã€‚ åœ¨ç¬¬ä¸‰èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†æˆ‘ä»¬è€ƒè™‘çš„ç³»ç»Ÿç±»åˆ«ï¼Œä»¥åŠç›¸å…³çš„é€†æœ€ä¼˜æ§åˆ¶é—®é¢˜ã€‚
	     åœ¨ç¬¬å››èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†é€†å‘æœ€ä¼˜æ§åˆ¶çš„ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸æˆ‘ä»¬çš„æ–°æ–¹æ³• [1]-[3] è¿›è¡Œäº†æ¯”è¾ƒã€‚ åœ¨ç¬¬äº”èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®æœ€ä¼˜æ§åˆ¶çš„å¿…è¦æ¡ä»¶å¼€å‘äº†æˆ‘ä»¬çš„æ–°æ–¹æ³•ã€‚ åœ¨ç¬¬å…­èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†æ¨¡æ‹Ÿ e
-
# II. RELATED WORK
- Inverse optimal control is often used as a solution approach to the broad problem of learning from demonstration, which is often referred to as imitation learning or apprenticeship learning.
	- The problem of learning from demonstration is to derive a control policy (a mapping from states to actions) from examples, or demonstrations, provided by a teacher.
	- Demonstrations are typically considered to be sequences of state-action pairs recorded during the teacherâ€™s demonstration
	- CN
	  - é€†æœ€ä¼˜æ§åˆ¶é€šå¸¸è¢«ç”¨ä½œè§£å†³ä»ç¤ºèŒƒä¸­å­¦ä¹ çš„å¹¿æ³›é—®é¢˜çš„æ–¹æ³•ï¼Œè¿™é€šå¸¸è¢«ç§°ä¸ºæ¨¡ä»¿å­¦ä¹ æˆ–å­¦å¾’å­¦ä¹ ã€‚ ä»æ¼”ç¤ºä¸­å­¦ä¹ çš„é—®é¢˜æ˜¯ä»æ•™å¸ˆæä¾›çš„ç¤ºä¾‹æˆ–æ¼”ç¤ºä¸­å¯¼å‡ºæ§åˆ¶ç­–ç•¥ï¼ˆä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼‰ã€‚
	    æ¼”ç¤ºé€šå¸¸è¢«è®¤ä¸ºæ˜¯æ•™å¸ˆæ¼”ç¤ºæœŸé—´è®°å½•çš„çŠ¶æ€-åŠ¨ä½œå¯¹åºåˆ—
- There are generally two methods of approach within learning from demonstration.
	- One approach is to learn a map from states to actions using classification or regression.
	- Argall, et al. provide a survey of the work in this area [5].
	- The second general approach is to learn a cost function with respect to which observed input and state trajectories are (approximately) optimal, i.e. inverse optimal control [1]â€“[4], [6]â€“[21].
	- These methods have primarily focused on finite-dimensional optimization and stochastic optimal control problems
	- CN 
	  - ä»ç¤ºèŒƒä¸­å­¦ä¹ é€šå¸¸æœ‰ä¸¤ç§æ–¹æ³•ã€‚ ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨åˆ†ç±»æˆ–å›å½’æ¥å­¦ä¹ ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ã€‚
	    é˜¿åŠ å°”ç­‰äººã€‚ æä¾›å¯¹è¯¥é¢†åŸŸå·¥ä½œçš„è°ƒæŸ¥[5]ã€‚
	    ç¬¬äºŒç§é€šç”¨â€‹â€‹æ–¹æ³•æ˜¯å­¦ä¹ ä¸€ä¸ªä»£ä»·å‡½æ•°ï¼Œå…³äºè§‚å¯Ÿåˆ°çš„è¾“å…¥å’ŒçŠ¶æ€è½¨è¿¹æ˜¯ï¼ˆè¿‘ä¼¼ï¼‰æœ€ä¼˜çš„ï¼Œå³é€†æœ€ä¼˜æ§åˆ¶ [1]-[4]ï¼Œ[6]-[21]ã€‚ è¿™äº›æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æœ‰é™ç»´ä¼˜åŒ–å’Œéšæœºæœ€ä¼˜æ§åˆ¶é—®é¢˜ä¸Š
- In the context of finite parameter optimization, Keshavarz, et al. [4] develop an inverse optimization method which learns the value function of a discrete-time stochastic control system given observations.
	- These ideas were extended to learn a cost function for a deterministic discrete-time system in Puydupin-Jamin, et al. [6], and a hybrid dynamical system in [22].
	- Similarly, Terekhov, et al. [7], [8] and Park, et al.
	  [9] develop an inverse optimization method for deterministic finite-dimensional optimization problems with additive cost functions and linear constraints.
	- Other recent work formulates an optimization problem which simultaneously learns a cost function and optimal trajectories [23], [24].
	- cn
	  - åœ¨æœ‰é™å‚æ•°ä¼˜åŒ–çš„èƒŒæ™¯ä¸‹ï¼ŒKeshavarz ç­‰äººã€‚  [4] å¼€å‘äº†ä¸€ç§é€†ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å­¦ä¹ ç»™å®šè§‚æµ‹å€¼çš„ç¦»æ•£æ—¶é—´éšæœºæ§åˆ¶ç³»ç»Ÿçš„å€¼å‡½æ•°ã€‚ è¿™äº›æƒ³æ³•åœ¨ Puydupin-Jamin ç­‰äººä¸­è¢«æ‰©å±•åˆ°å­¦ä¹ ç¡®å®šæ€§ç¦»æ•£æ—¶é—´ç³»ç»Ÿçš„æˆæœ¬å‡½æ•°ã€‚  [6] å’Œ [22] ä¸­çš„æ··åˆåŠ¨åŠ›ç³»ç»Ÿã€‚ åŒæ ·ï¼Œæ·åˆ—éœå¤«ç­‰äººã€‚  [7]ã€[8] å’Œ Park ç­‰äººã€‚
	     [9] å¼€å‘äº†ä¸€ç§ç”¨äºå…·æœ‰é™„åŠ æˆæœ¬å‡½æ•°å’Œçº¿æ€§çº¦æŸçš„ç¡®å®šæ€§æœ‰é™ç»´ä¼˜åŒ–é—®é¢˜çš„é€†ä¼˜åŒ–æ–¹æ³•ã€‚ æœ€è¿‘çš„å…¶ä»–å·¥ä½œåˆ¶å®šäº†ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œè¯¥é—®é¢˜åŒæ—¶å­¦ä¹ æˆæœ¬å‡½æ•°å’Œæœ€ä½³è½¨è¿¹ [23]ã€[24]ã€‚
- A variety of methods have been developed in the context of stochastic optimal control problems and, in particular, Markov decision processes [1], [11], [13], [15]â€“[17], [19], [20].
	- Ng and Russell [11] developed a method for stationary Markov decision processes based on linear programming.
	- Abbeel and Ng [1] extends that work by finding a cost function with respect to which the expertâ€™s cost is less than that of predicted trajectories by a margin.
	- A further extension simultaneously learns the system dynamics along specific trajectories of interest [13].
	- Ramachandran, et al. [16] takes a Bayesian approach and assumes that actions are distributed proportional to the future expected reward.
	- The method developed in Ziebart, et al. [17] works by computing a probability distribution over all possible paths which matches features along the observed trajectory.
	- Dvjijotham and Todorov [19] develop a method of inverse optimal control for linearlysolvable stochastic optimal control problems.
	- Their method takes advantage of the fact that, for the class of system model they consider, the Hamilton-Jacobi-Bellman equation gives an explicit formula for the cost function once the value function is known.
	- Aghasadeghi and Bretl [20] develop a method of inverse optimal control that uses path integrals to create a probability distribution over all possible paths.
	- The problem is then one of maximizing the likelihood of observations.
	- cn
	  - å·²ç»åœ¨éšæœºæœ€ä¼˜æ§åˆ¶é—®é¢˜çš„èƒŒæ™¯ä¸‹å¼€å‘äº†å¤šç§æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ [1]ã€[11]ã€[13]ã€[15]-[17]ã€[19]ã€[20]  .  Ng å’Œ Russell [11] å¼€å‘äº†ä¸€ç§åŸºäºçº¿æ€§è§„åˆ’çš„å¹³ç¨³é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„æ–¹æ³•ã€‚
	     Abbeel å’Œ Ng [1] é€šè¿‡æ‰¾åˆ°ä¸€ä¸ªæˆæœ¬å‡½æ•°æ¥æ‰©å±•è¿™é¡¹å·¥ä½œï¼Œåœ¨è¯¥å‡½æ•°ä¸­ï¼Œä¸“å®¶çš„æˆæœ¬æ¯”é¢„æµ‹è½¨è¿¹çš„æˆæœ¬ä½ä¸€ç‚¹ã€‚ è¿›ä¸€æ­¥çš„æ‰©å±•åŒæ—¶å­¦ä¹ æ²¿ç‰¹å®šæ„Ÿå…´è¶£è½¨è¿¹çš„ç³»ç»ŸåŠ¨åŠ›å­¦ [13]ã€‚ æ‹‰é©¬é’±å¾·å…°ç­‰äººã€‚  [16] é‡‡ç”¨è´å¶æ–¯æ–¹æ³•ï¼Œå¹¶å‡è®¾è¡ŒåŠ¨çš„åˆ†å¸ƒä¸æœªæ¥çš„é¢„æœŸå›æŠ¥æˆæ­£æ¯”ã€‚  Ziebart ç­‰äººå¼€å‘çš„æ–¹æ³•ã€‚  [17] çš„å·¥ä½œåŸç†æ˜¯è®¡ç®—æ‰€æœ‰å¯èƒ½è·¯å¾„ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™äº›è·¯å¾„ä¸æ²¿è§‚å¯Ÿè½¨è¿¹çš„ç‰¹å¾åŒ¹é…ã€‚  Dvjijotham å’Œ Todorov [19] å¼€å‘äº†ä¸€ç§ç”¨äºçº¿æ€§å¯è§£éšæœºæœ€ä¼˜æ§åˆ¶é—®é¢˜çš„é€†æœ€ä¼˜æ§åˆ¶æ–¹æ³•ã€‚ ä»–ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼šå¯¹äºä»–ä»¬è€ƒè™‘çš„ç³»ç»Ÿæ¨¡å‹ç±»åˆ«ï¼Œä¸€æ—¦ä»·å€¼å‡½æ•°å·²çŸ¥ï¼ŒHamilton-Jacobi-Bellman æ–¹ç¨‹å°±ä¼šç»™å‡ºæˆæœ¬å‡½æ•°çš„æ˜ç¡®å…¬å¼ã€‚  Aghasadeghi å’Œ Bretl [20] å¼€å‘äº†ä¸€ç§é€†ä¼˜åŒ–æ§åˆ¶æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è·¯å¾„ç§¯åˆ†æ¥åˆ›å»ºæ‰€æœ‰å¯èƒ½è·¯å¾„çš„æ¦‚ç‡åˆ†å¸ƒã€‚
	     é‚£ä¹ˆé—®é¢˜ä¹‹ä¸€æ˜¯æœ€å¤§åŒ–è§‚å¯Ÿçš„å¯èƒ½æ€§ã€‚
-
- Learning from demonstration methods are applied in three different areas.
	- First, learning from demonstration has been applied as a method of data-driven automation [1], [2], [11]â€“ [15], [17], [19], [25]â€“[27].
	- Tasks of interest include bipedal walking, navigation of aircraft, operation of agricultural and construction vehicles.
	- Second, learning from demonstration methods have been applied to cognitive and neural modeling [3], [7]â€“[10], [18], [28]â€“[31].
	- Third, learning from demonstration methods have been applied to system identification of deformable objects [21], i.e. learning elastic stiffness parameters of objects such as surgical suture, rope, and hair.
	- cn
	  - ä»ç¤ºèŒƒæ–¹æ³•ä¸­å­¦ä¹ è¢«åº”ç”¨äºä¸‰ä¸ªä¸åŒçš„é¢†åŸŸã€‚ é¦–å…ˆï¼Œä»æ¼”ç¤ºä¸­å­¦ä¹ å·²è¢«ç”¨ä½œæ•°æ®é©±åŠ¨è‡ªåŠ¨åŒ–çš„ä¸€ç§æ–¹æ³• [1]ã€[2]ã€[11]-[15]ã€[17]ã€[19]ã€[25]-[27]ã€‚ æ„Ÿå…´è¶£çš„ä»»åŠ¡åŒ…æ‹¬åŒè¶³è¡Œèµ°ã€é£æœºå¯¼èˆªã€å†œä¸šå’Œå·¥ç¨‹è½¦è¾†çš„æ“ä½œã€‚ å…¶æ¬¡ï¼Œä»æ¼”ç¤ºæ–¹æ³•ä¸­å­¦ä¹ å·²åº”ç”¨äºè®¤çŸ¥å’Œç¥ç»å»ºæ¨¡ [3]ã€[7]-[10]ã€[18]ã€[28]-[31]ã€‚ ç¬¬ä¸‰ï¼Œä»æ¼”ç¤ºæ–¹æ³•ä¸­å­¦ä¹ å·²åº”ç”¨äºå¯å˜å½¢ç‰©ä½“çš„ç³»ç»Ÿè¯†åˆ« [21]ï¼Œå³å­¦ä¹ æ‰‹æœ¯ç¼åˆçº¿ã€ç»³ç´¢å’Œå¤´å‘ç­‰ç‰©ä½“çš„å¼¹æ€§åˆšåº¦å‚æ•°ã€‚
# III. INVERSE OPTIMAL CONTROL: PROBLEM STATEMENT
- In the rest of this paper, we consider the following class of optimal control problems
- (1)
- where x(t) âˆˆ X âŠ‚ Rn is the state, u(t) âˆˆ U âŠ‚ Rm is the input, Ï† : R Ã— X Ã— U â†’ Rk+ are known basis functions, andc âˆˆ Rk is an unknown parameter vector to be learned.
	- We assume, without loss of generality, that kck â‰¤ 1.
	- We assume that the system equations
- (2)
- are ^^well posed^^, that is, for every initial condition xstart and every admissible control u(t), the system xË™(t) =f[x(t), u(t)] has a unique solution x on t âˆˆ [0, tf ].
	- This is satisfied, for example, when f is continuous in t and u
	  and differentiable (C1) in x, fx is continuous in t and u, and
	  u is piecewise continuous as a function of t [32], [33].
	- The objective basis function Ï† is assumed to be smooth in x and
	  u.
	- This problem also assumes there are no input and state
	  constraints.
	- These constraints are often important in practice, and will be the subject of future work.
- The problem of inverse optimal control is to infer the unknown parameters with respect to which a given trajectory, the observation, is a local extremal solution to problem (1).
	- This observed trajectory is denoted by
	- cn
	  - é€†æœ€ä¼˜æ§åˆ¶çš„é—®é¢˜æ˜¯æ¨æ–­æœªçŸ¥å‚æ•°ï¼Œå¯¹äºè¿™äº›æœªçŸ¥å‚æ•°ï¼Œç»™å®šè½¨è¿¹ï¼Œå³è§‚æµ‹å€¼ï¼Œæ˜¯é—®é¢˜ (1) çš„å±€éƒ¨æå€¼è§£ã€‚
	     è¿™ä¸ªè§‚å¯Ÿåˆ°çš„è½¨è¿¹è¡¨ç¤ºä¸º
- (3)
- For convenience, we will often drop the asterisk and refer to an optimal trajectory as (x, t).
	- We also consider observing multiple trajectories, each local minima of problem (1) for different boundary conditions.
	- We will refer to a set D of M observations as follows
	- c 
	  - ä¸ºæ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå»æ‰æ˜Ÿå·å¹¶å°†æœ€ä½³è½¨è¿¹ç§°ä¸º (x, t)ã€‚ æˆ‘ä»¬è¿˜è€ƒè™‘è§‚å¯Ÿå¤šä¸ªè½¨è¿¹ï¼Œé’ˆå¯¹ä¸åŒè¾¹ç•Œæ¡ä»¶çš„é—®é¢˜ (1) çš„æ¯ä¸ªå±€éƒ¨æœ€å°å€¼ã€‚ æˆ‘ä»¬å°†å‚è€ƒ M ä¸ªè§‚å¯Ÿç»“æœçš„é›†åˆ D å¦‚ä¸‹
- (4)
- where each trajectory has boundary conditions
  - å…¶ä¸­æ¯ä¸ªè½¨è¿¹éƒ½æœ‰è¾¹ç•Œæ¡ä»¶
- (5)
- An important quantity in the methods discussed in this paper is the accumulated value of the unweighted basis functions along a trajectory, which we call the feature vector of a trajectory Âµ(x, u), and define by
  - æœ¬æ–‡è®¨è®ºçš„æ–¹æ³•ä¸­çš„ä¸€ä¸ªé‡è¦é‡æ˜¯æ²¿è½¨è¿¹çš„æœªåŠ æƒåŸºå‡½æ•°çš„ç´¯åŠ å€¼ï¼Œæˆ‘ä»¬ç§°å…¶ä¸ºè½¨è¿¹çš„ç‰¹å¾å‘é‡ Î¼(x, u)ï¼Œå®šä¹‰ä¸º
- (6)
- In practice, one would generally have sampled observations of the behavior of the system, but for the analysis in this paper, we assume we have perfect observations of the continuous-time system trajectories.
  - åœ¨å®è·µä¸­ï¼Œäººä»¬é€šå¸¸ä¼šå¯¹ç³»ç»Ÿè¡Œä¸ºè¿›è¡Œé‡‡æ ·è§‚å¯Ÿï¼Œä½†å¯¹äºæœ¬æ–‡ä¸­çš„åˆ†æï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬å¯¹è¿ç»­æ—¶é—´ç³»ç»Ÿè½¨è¿¹æœ‰å®Œç¾çš„è§‚å¯Ÿã€‚
- In practice, the solution of (1) is typically obtained using
  a numerical optimal control solver such as direct multiple
  shooting or collocation.
	- In this paper, we use the pseudospectral optimal control package GPOPS [34] to numerically solve the forward problem in the prior methods which require it.
	- c
	  - åœ¨å®è·µä¸­ï¼Œï¼ˆ1ï¼‰çš„è§£é€šå¸¸æ˜¯ä½¿ç”¨æ•°å€¼ä¼˜åŒ–æ§åˆ¶æ±‚è§£å™¨è·å¾—çš„ï¼Œä¾‹å¦‚ç›´æ¥å¤šé‡å°„å‡»æˆ–æ­é…ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¼ªè°±æœ€ä¼˜æ§åˆ¶åŒ… GPOPS [34] æ¥æ•°å€¼æ±‚è§£ç°æœ‰æ–¹æ³•ä¸­éœ€è¦å®ƒçš„å‰å‘é—®é¢˜ã€‚
# IV. THREE PRIOR METHODS
- In this section we formally describe the three prior methods of inverse optimal control with which we compare the new method developed in Section V.
	- In their original form, the method of Abbeel and Ng, and the method of Ratliff, et al. were developed in the context of Markov decision processes.
	- The general structure and theoretical guarantees of the methods apply with slight modification to the deterministic continuous-time class of problems we consider in this paper, specified in Equation (1).
	- c
	  - åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ­£å¼æè¿°äº†é€†æœ€ä¼˜æ§åˆ¶çš„ä¸‰ç§å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬å°†å®ƒä»¬ä¸ç¬¬äº”éƒ¨åˆ†ä¸­å¼€å‘çš„æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨å®ƒä»¬çš„åŸå§‹å½¢å¼ä¸­ï¼ŒAbbeel å’Œ Ng çš„æ–¹æ³•ä»¥åŠ Ratliff ç­‰äººçš„æ–¹æ³•ã€‚ æ˜¯åœ¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„èƒŒæ™¯ä¸‹å¼€å‘çš„ã€‚ è¿™äº›æ–¹æ³•çš„ä¸€èˆ¬ç»“æ„å’Œç†è®ºä¿è¯é€‚ç”¨äºæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­è€ƒè™‘çš„ç¡®å®šæ€§è¿ç»­æ—¶é—´ç±»é—®é¢˜ï¼Œåœ¨ç­‰å¼ï¼ˆ1ï¼‰ä¸­æŒ‡å®šã€‚
## A. Method of Mombaur, et al.
- The method of Mombaur, et al. [3] works by searching for the cost function parameter c which minimizes the sum-squared error between predicted and observed trajectories.
	- This method has two main components.
	- In the upper-level, a derivative-free optimization technique is used to search for the cost function parameter c.
	- In the lower-level, a numerical optimal control method is used to solve the forward optimal control problem (1) for a candidate value of c.
	- We will now discuss the two levels in detail.
	- c
	  -   Mombaur ç­‰äººçš„æ–¹æ³•ã€‚  [3] é€šè¿‡æœç´¢æˆæœ¬å‡½æ•°å‚æ•° c æ¥æœ€å°åŒ–é¢„æµ‹å’Œè§‚å¯Ÿåˆ°çš„è½¨è¿¹ä¹‹é—´çš„æ€»å’Œè¯¯å·®ã€‚
	     è¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚ 
	  åœ¨ä¸Šå±‚ï¼Œä½¿ç”¨æ— å¯¼æ•°ä¼˜åŒ–æŠ€æœ¯æ¥æœç´¢æˆæœ¬å‡½æ•°å‚æ•° cã€‚ 
	  åœ¨ä¸‹å±‚ï¼Œé‡‡ç”¨æ•°å€¼æœ€ä¼˜æ§åˆ¶æ–¹æ³•æ±‚è§£cçš„å€™é€‰å€¼çš„å‰å‘æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼ˆ1ï¼‰ã€‚ 
	  æˆ‘ä»¬ç°åœ¨å°†è¯¦ç»†è®¨è®ºè¿™ä¸¤ä¸ªçº§åˆ«ã€‚
- The objective of the upper-level derivative-free optimization is given by the following
  - ä¸Šå±‚æ— å¯¼æ•°ä¼˜åŒ–çš„ç›®æ ‡ç”±ä¸‹å¼ç»™å‡º
- (7)
- where [x âˆ— (t); u âˆ— (t)] is the vector concatenation of the state and input of the observed trajectory at time t, and [x c (t); u c (t)] is the solution to the forward problem (1), given the parameter vector c.
	- Mombaur, et al. [3] discuss high performance derivative-free algorithms to minimize this upper-level objective.
	- For our baseline analysis in this paper, however, we use the Matlab `fminsearch` implementation of the Nelder-Mead simplex algorithm.
	- Iterations of the Nelder-Mead algorithm constitute the upper-level of this method.
	- Upon selecting a new candidate value of c, the lower-level proceeds by solving (1) for the candidate value, generating a predicted trajectory (x c , uc ).
	- Given this trajectory, the upper-level objective can be evaluated, and the search for a new candidate c continues.
	- This method is easily extended for the case where multiple trajectories are observed by considering the sum of predicted errors with respect to each observed trajectory in the upper-level objective.
	- c
	  - å…¶ä¸­ [x * (t);  u âˆ— (t)] æ˜¯åœ¨æ—¶é—´ t æ—¶è§‚æµ‹è½¨è¿¹çš„çŠ¶æ€å’Œè¾“å…¥çš„å‘é‡çº§è”ï¼Œ[x c (t);  u c (t)] æ˜¯å‰å‘é—®é¢˜ (1) çš„è§£ï¼Œç»™å®šå‚æ•°å‘é‡ cã€‚ è’™é²å°”ç­‰äººã€‚  [3] è®¨è®ºé«˜æ€§èƒ½æ— å¯¼æ•°ç®—æ³•ä»¥æœ€å°åŒ–è¿™ä¸ªä¸Šå±‚ç›®æ ‡ã€‚ ç„¶è€Œï¼Œå¯¹äºæœ¬æ–‡ä¸­çš„åŸºçº¿åˆ†æï¼Œæˆ‘ä»¬ä½¿ç”¨äº† Nelder-Mead å•çº¯å½¢ç®—æ³•çš„ Matlab fminsearch å®ç°ã€‚  Nelder-Mead ç®—æ³•çš„è¿­ä»£æ„æˆäº†è¯¥æ–¹æ³•çš„ä¸Šå±‚ã€‚ åœ¨é€‰æ‹© c çš„æ–°å€™é€‰å€¼åï¼Œä¸‹å±‚é€šè¿‡å¯¹å€™é€‰å€¼æ±‚è§£ (1)ï¼Œç”Ÿæˆé¢„æµ‹è½¨è¿¹ (x c , uc )ã€‚ ç»™å®šè¿™ä¸ªè½¨è¿¹ï¼Œå¯ä»¥è¯„ä¼°ä¸Šå±‚ç›®æ ‡ï¼Œå¹¶ç»§ç»­æœç´¢æ–°çš„å€™é€‰ cã€‚ é€šè¿‡è€ƒè™‘ç›¸å¯¹äºä¸Šå±‚ç›®æ ‡ä¸­æ¯ä¸ªè§‚å¯Ÿåˆ°çš„è½¨è¿¹çš„é¢„æµ‹è¯¯å·®æ€»å’Œï¼Œè¯¥æ–¹æ³•å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°è§‚å¯Ÿåˆ°å¤šä¸ªè½¨è¿¹çš„æƒ…å†µã€‚
## B. Method of Abbeel and Ng
- The method of Abbeel and Ng [1] was originally developed for infinite-horizon Markov decision processes with discounted reward.
	- The goal of this method is to find a control policy which yield a feature vector close to that of the observed trajectory.
	- The method is initialized by selecting a random cost function parameter vector c (0) and solving the forward problem (1) to obtain an initial predicted trajectory (x (0), u(0)) and associated feature vector Âµ (0).
	- On the i-th iteration, solve the following quadratic program:
	- c
	  - Abbeel å’Œ Ng çš„æ–¹æ³• [1] æœ€åˆæ˜¯ä¸ºå…·æœ‰æŠ˜æ‰£å¥–åŠ±çš„æ— é™èŒƒå›´é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹è€Œå¼€å‘çš„ã€‚ è¯¥æ–¹æ³•çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç§æ§åˆ¶ç­–ç•¥ï¼Œè¯¥ç­–ç•¥äº§ç”Ÿçš„ç‰¹å¾å‘é‡ä¸è§‚å¯Ÿåˆ°çš„è½¨è¿¹çš„ç‰¹å¾å‘é‡æ¥è¿‘ã€‚ è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©ä¸€ä¸ªéšæœºä»£ä»·å‡½æ•°å‚æ•°å‘é‡c(0)å¹¶æ±‚è§£å‰å‘é—®é¢˜(1)æ¥åˆå§‹åŒ–ï¼Œå¾—åˆ°åˆå§‹é¢„æµ‹è½¨è¿¹(x(0)ï¼Œu(0))å’Œç›¸å…³ç‰¹å¾å‘é‡Î¼(0)ã€‚ åœ¨ç¬¬ i æ¬¡è¿­ä»£ä¸­ï¼Œæ±‚è§£ä»¥ä¸‹äºŒæ¬¡ç¨‹åºï¼š
- (8)
- where b (i) is the margin on the i-th iteration, and Âµ âˆ— and Âµ (j) are the feature vectors of the optimal trajectory and jth trajectory, respectively (recall the definition given in (6)).
	- If b (i) < , then terminate.
	- Otherwise, given the result from the quadratic program, c (i) , solve the forward optimal control problem, Equation (1), with c = c (i) to obtain the predicted trajectory (x (i) , u(i) ) and associated feature vector Âµ (i) .
	- Set i = i + 1 and repeat.
	- c
	  - å…¶ä¸­ b (i) æ˜¯ç¬¬ i æ¬¡è¿­ä»£çš„ä½™é‡ï¼ŒÎ¼ âˆ— å’Œ Î¼ (j) åˆ†åˆ«æ˜¯æœ€ä¼˜è½¨è¿¹å’Œç¬¬ j ä¸ªè½¨è¿¹çš„ç‰¹å¾å‘é‡ï¼ˆå›å¿†ï¼ˆ6ï¼‰ä¸­ç»™å‡ºçš„å®šä¹‰ï¼‰ã€‚
	     å¦‚æœ b (i) < ï¼Œåˆ™ç»ˆæ­¢ã€‚ å¦åˆ™ï¼Œç»™å®šäºŒæ¬¡è§„åˆ’çš„ç»“æœ c (i) ï¼Œæ±‚è§£å‰å‘æœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œæ–¹ç¨‹ (1)ï¼Œå…¶ä¸­ c = c (i) ä»¥è·å¾—é¢„æµ‹è½¨è¿¹ (x (i) , u(i) ) å’Œç›¸å…³çš„ç‰¹å¾å‘é‡ Âµ (i) ã€‚ è®¾ç½® i = i + 1 å¹¶é‡å¤ã€‚
- As shown in [1], this method terminates after a finite number of iterations (the theorems stated and proved in [1] carry over with minor modification to the deterministic continuous-time case, which we omit for brevity).
	- Upon termination, this algorithm returns a set of policies Î , and there exists at least one policy in Î  that yields a feature vector differing from the expertâ€™s by no more than .
	- c
	  - å¦‚ [1] æ‰€ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æœ‰é™æ¬¡æ•°çš„è¿­ä»£åç»ˆæ­¢ï¼ˆ[1] ä¸­é™ˆè¿°å’Œè¯æ˜çš„å®šç†åœ¨å¯¹ç¡®å®šæ€§è¿ç»­æ—¶é—´æƒ…å†µç¨åŠ ä¿®æ”¹åå¾—ä»¥å»¶ç»­ï¼Œä¸ºç®€æ´èµ·è§ï¼Œæˆ‘ä»¬å°†å…¶çœç•¥ï¼‰ã€‚ åœ¨ç»ˆæ­¢æ—¶ï¼Œè¯¥ç®—æ³•è¿”å›ä¸€ç»„ç­–ç•¥ Î ï¼Œå¹¶ä¸”åœ¨ Î  ä¸­è‡³å°‘å­˜åœ¨ä¸€ä¸ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥äº§ç”Ÿä¸ä¸“å®¶çš„ç‰¹å¾å‘é‡ç›¸å·®ä¸è¶…è¿‡ çš„ç‰¹å¾å‘é‡ã€‚
-
-
-
-
-