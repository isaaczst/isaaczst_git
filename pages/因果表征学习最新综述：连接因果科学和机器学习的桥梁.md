- 因果表征学习最新综述：连接因果科学和机器学习的桥梁 - 集智科学家的文章 - 知乎
  https://zhuanlan.zhihu.com/p/355009051
- 章
  导语
  处于信息时代的我们，有幸经历了轰轰烈烈的以数据为中心的大数据革命（涉及机器学习，深度学习及其应用，例如 Alpha-Go, GPT-3, 自动驾驶等），深刻改变了我们生活的方方面面。如今另外一场相对不那么广为人知，但是同样重要的因果革命正在进行，它以因果科学中心并席卷了各个领域，尤其是人工智能。近日，一篇探索让 AI 系统攀登因果之梯的深刻综述文章“Torwards Causal Representation Learning”引起了大家广泛的关注。该文章可以视作 Bernhard Schölkopf 2019 年的 Judea Pearl 亲自点赞文章 “Causality for Machine Learning ”的姐妹篇，并且结合了 Yoshua Bengio 等人在表征学习上的深度思考，是因果结合机器学习的必读佳作，是2021年因果表征学习的第一课。
- 这篇名为Towards Causal Representation Learning的综述文章由因果领域领军人物马普智能系统所所长Bernhard Schölkopf及深度学习三巨头之一的Yoshua Bengio等人撰写。顾名思义，何为因果表征causal representation呢？其又拥有怎样优良的性质使我们想要学习因果表征呢？
- 为了回答这些问题，让我们从表征学习说起。表征学习Representation Learning是机器学习中的重要问题，良好的表征是机器学习算法成功的重要条件；正因如此，近十年来深度学习借助神经网络强大的表达能力、海量的数据以及强大的算力，自动地从数据中学习表征，取代了传统的人工制作的特征，取得了瞩目的成就。由深度学习三巨头之二的Yoshua Bengio和Yann LeCun牵头创办的深度学习顶会，即命名为国际学习表征会议International Conference on Learning Representations，足见表征学习的重要性。
- 尽管深度学习在近十年极大地推动了机器学习的发展，但是仍有许多问题亟待解决，例如将知识迁移到新问题上的能力。许多关键问题都可以归结为OOD(out-of-distribution)问题。因为统计学习模型需要独立同分布(i.i.d.)假设，若测试数据与训练数据来自不同的分布，统计学习模型往往会出错。然而在很多情况下，i.i.d.的假设是不成立的，而因果推断所研究的正是这样的情形：如何学习一个可以在不同分布下工作、蕴含因果机制的因果模型(Causal Model)，并使用因果模型进行干预或反事实推断。
- 我们可以很自然地想到将因果推断的优点结合到机器学习中，然而现实没有这么容易。因果模型往往处理的是结构化的数据，并不能处理机器学习中常见的高维的低层次的原始数据，例如图像。为此，让我们回到最初的问题，因果表征即可理解为可以用于因果模型的表征，因果表征学习即为将图像这样的原始数据转化为可用于因果模型的结构化变量。因果表征学习就是连接因果科学与机器学习的桥梁，解决这一及相关问题，就可以很好地将因果推断与机器学习结合起来，构建下一代更强大的AI。
- 接下来，我们就跟随作者的思路，详细地探讨描述物理世界的不同模型的层次，统计模型与因果模型的区别包括模型的能力相关的假设及挑战，学习因果模型所必需的独立因果机制原则，学习因果模型的方法，如何学习因果表征，以及从因果的角度重新审视了诸多机器学习的挑战并指出了因果带来的启示。
- Level of causal modeling
  谈及对自然现象建模，自然而然就可想到黄金标准——微分方程组。它根据时间的演变建模物理机制，可以让我们预测物理系统未来的行为，推断干预的效果以及预测变量间的统计相关性；还可以提供物理本质，让我们可以解读因果结构。
- 如果说微分方程是对物理系统全面详尽的表述，那么统计模型(Statistical Model)可被看作表面的粗糙的描述。它无法预测干预的效果，但是的优点在于通常可以从观察数据中学习，而前者通常需要专家来提出。因果建模则存在于这两个极端之间，它期望能够像物理模型一样预测干预的效果，但同时可以在一些假设下，通过数据驱动的方法找到这样的模型，来取代专家知识。
- 基于上文的表述，表一给出了模型的分类与层级，并且给出了分级的依据——越高层的模型拥有更多更强的能力，这些能力从低到高分别是：在i.i.d.条件下预测的能力，在分布偏移/干预下预测的能力，回答反事实问题的能力，是否蕴含物理本质。接下来首先讨论这些能力，并在下一个章节具体解析统计模型与因果模型的区别。具体地说，基于统计模型的机器学习模型只能建模相关关系，而相关关系往往会随着数据分布的变化而变化；而因果模型所建模的因果关系则是更本质的，反映数据生成机制的关系，这样的关系是更鲁棒的，具有OOD泛化的能力。
- 在独立同分布条件下预测的能力
- 统计模型只是对现实的粗浅描述，因为它们只关注关联关系。对于样本和标签，我们可以通过估计来回答这样的问题：“这张特定照片中中有狗的概率是多少？”，“给定一些症状，心力衰竭的概率是多少？”。这样的问题是可以通过观察足够多的)产生的i.i.d.数据来回答的。尽管机器学习算法可以把这些事做得很好，但是准确的预测对于我们的决策是不够，而因果科学提供了一个尚未完全探索的补充。举例来说，鹳出现的频率是和欧洲的人口出生率正相关的，我们的确可以训练一个统计学习模型来通过鹳的频率预测出生率，但显然这两者并没有什么直接的因果关系。统计模型只有在i.i.d.的情况下才是准确的，如果我们做任何的干预来改变数据分布，就会导致统计学习模型出错。
- 在分布偏移/干预条件下预测的能力
- 我们进一步讨论干预问题，它是更具挑战性的，因为干预会使我们跳出统计学习中i.i.d.的假设。继续用鹳的例子，“在一个国家中增加鹳的数量会增加该国的出生率吗？”就是一个干预问题。显然，人为的干预会使得数据分布发生变化，统计学习依赖的条件就会被打破，所以它会失效；另一方面，如果我们可以在干预的情况下学习一个预测模型，那么这有可能让我们得到一个对现实环境中的分布变化鲁棒的模型。实际上这里所谓的干预并不是什么新鲜事，很多事情本身就是随时间变化的，例如人的兴趣偏好，或者模型的训练集与测试集本身就有分布的不匹配。对神经网络的鲁棒性，已经有越来越多的关注，成为了一个与因果推断紧密连接的研究话题。作者认为对于在分布偏移下预测的研究不能只局限于在测试集上取得高准确率，如果我们希望在实际决策中使用学习算法，那么我们必须相信在实验条件改变的情况下，模型的预测也是有效的。笔者认为，作者在此处的意思是，实际应用中的分布偏移是任意多样的，仅仅在某些测试集上取得好效果不能代表我们可以在任何情况下都信任该模型，它可能只是恰好符合这些测试集的偏置。
- 为了使我们可以在尽可能多的情况下信任预测模型，就要采用具有回答干预问题能力的模型，至少统计学习模型是不行的。
- 回答反事实问题的能力
- 反事实问题涉及到推理事情为什么会发生，想象不同行为的后果，并由此可以决定采取何种行为来达到期望的结果。回答反事实问题更加困难的，但也是对于AI非常关键的挑战。如果一个干预问题是“如果我们说服一个病人规律的锻炼，那么它心力衰竭的概率会如何变化？”，那么对应的反事实问题就是“如果这个已经心力衰竭的病人一年前就开始锻炼，那他还会心力衰竭吗？”。显然回答这样的反事实问题对于强化学习中的智能体是很重要的，它们可以通过反思自己的决策，制定假说，再通过实践验证，就像我们的科学研究一样。
- 数据的特点：观察的—干预的，结构化的-非结构化的
- 数据的形式往往决定了我们可以推断什么样的关系。作者将数据分为两个维度：观察的与因果的，人工总结的（结构化的）与原始的（非结构化的）。
- 观察的与干预的数据
  一个极端情况是，我们常假设的从同一个分布中i.i.d.采样的观察数据，但这样的条件很少能被严格的满足；另一个极端是在已知的不同的干预下的产生的数据。在这两者之间，是偏移或者干预未知的数据。
- 结构化的与非结构化的数据
  在传统的AI中，数据常被假设为高层有语义的结构化变量，它们有些可能对应着潜在图中的因果变量。而非结构化原始数据是指那些无法直接提供因果信息的数据，例如图像。
- 尽管统计模型比因果模型要弱，但是它们可以同时有效地在结构化或者非结构化学习。另一方面，尽管只从观察数据中学习因果关系的方法是存在的，但常常还是需要从多个环境中收集数据，或者需要能够做干预。至此，我们已经可以一窥问题的核心：因果模型具有回答干预问题和反事实问题的能力，然而只能用于结构化的数据，ML模型虽然可以从raw data中有效的学习，但却逃不出i.i.d.设定的桎梏，那么如何将两者结合，使机器学习突破当前的瓶颈呢？答案即如本文题目所言Towards Causal Representation Learning！因果表征学习，即从非结构化的数据中提取出可以用于因果推断的结构化变量。一言以蔽之，如果解决了因果表征学习的问题，就克服了因果推断领域和机器学习领域间的最关键障碍，就可构建下一代更强大的AI。
- 愿景虽好，脚踏实地才可为其提供保证，在接下来的章节中，作者一步一个脚印地介绍了因果模型和相关假设及挑战、估计因果关系的必要机制、传统的和与神经网络结合的因果发现方法、学习因果变量(表征)的关键问题，最后用因果的语言讨论了对机器学习领域诸多问题的启示。
- 因果模型和推断
  独立同分布数据驱动的方法
- 我们还是从传统机器学习模型谈起，机器学习的成功有4个重要的因素：(1)大量基于模拟器或人工标注的数据，(2)强大的机器学习系统，如神经网络，(3)高性能计算系统，这对因果推断也至关重要，(4)问题是i.i.d.的。
- 对于i.i.d.的数据，统计学习理论对模型提供了强大的保证，因此取得超越人类的表现也不足为奇。但是却在对人类很简单的不满足i.i.d.的情景下表现很差，即在不同问题间迁移的能力。
- 为了进一步理解，i.i.d.带来的问题，让我们考虑下面的例子。Alice在想在网上买一个笔记本电脑包，网上商店的推荐系统于是向Alice推荐了笔记本电脑。这个推荐看起来很不合理，因为很可能Alice是已经买了电脑才去买包。假设该网站推荐系统使用统计模型仅仅基于统计相关性来推荐，那么我们已知事件“Alice买了包”对于事件“Alice是否会买电脑”的不确定性减少，和已知事件“Alice买了电脑”对于事件“Alice是否会买包”的不确定性减少是相等的，都为两个随机事件的互信息。这就导致我们丢失了重要的方向信息，即买电脑往往导致买包。
- The Reichenbach Principle：从统计到因果
- 说了这么多，如何实现从统计相关到因果的跨越呢？Reichenbach清晰地阐述了二者的联系：
- Common Cause Principle：如果两个可观察量X和Y是统计相关的，那么一定存在一个变量因果的影响X和Y，并且可以解释它们之间全部的相关性，即给定Z，X和Y是条件独立的。
  这里需要注意的是，上述原理包含Z与X或Y重合的特殊情况。沿用前文鹳的例子，鹳的频率为X，出生率为Y，二者统计相关。如果鹳能带来孩子则是X→Y，孩子会吸引鹳则是X<—Y，有其他因素导致两者则为X<—Z—>Y。没有额外的假设，我们不能从观察数据中根据统计相关性区分这三种情况，所以因果模型比统计模型包含更多信息。
- 尽管只有两个变量情况下的因果发现很困难，但是在有更多变量的情况下则会简单很多，因为多变量情况下，因果图会蕴含更多条件独立性质。这会将Reichenbach Principle推广到接下来介绍的因果图模型或结构因果模型。
- 结构因果模型
  结构因果模型(Structural causal models, SCM)，是考虑一系列变量X1，，，Xn作为有向无环图(DAG)的顶点，每个变量值都由如下结构方程赋予
- 因果图模型
- 上述的DAG(被称为因果图，Causal Graph)，以及噪声的独立性蕴含了联合分布的规范分解，称之为因果(解耦)分解，(causal(disentangled) factorization)
- 当然其他的纠缠分解(entangeld factorization)也可能是成立的，例如
- 隐变量和混淆因子
  上述的图模型其实需要一个因果充分性假设，即不存在未观察到的共同原因变量。若该假设不满足，则会让因果推断变得很困难。因为它可能会让两个因果无关的可观测变量产生统计相关性，或者它们之间的因果关系被混淆因子的所污染(X是Y因，Z而是X,Y的共因，则Z被称为混淆因子(confounder))。这些是因果推断领域的重要问题，在此不多赘述。
- 干预
  所谓干预，即为修改SCM(1)中结构方程的一部分，例如改变Ui，设置fi(即Xi)为定值，或者改变fi的函数形式。
- 统计模型，因果图模型，结构因果模型的区别
- 另外如图1所示，因果图模型允许计算干预的分布，当一个变量被干预时，就把它设为固定值，并且切断与其父母节点的边，在新的图中计算出的即为该干预的分布。
- 结构因果模型则包含因果变量和带有独立噪声的结构方程，也可以计算干预分布，因为他可以表达成因果图模型的概率分解的形式；也可以进行反事实推理。在进行反事实推理的时候，我们需要噪声变量的值固定。
- 小结一下，统计学习的概念基础是联合分布P(Y,X1,,,,Xn)，期望通过特定模型在i.i.d.情况下学习E[Y|X]。因果学习(发现)需要考虑更多的假设，希望把联合分布进行因果分解，当得到因果模型后，就可以进行干预或反事实推理。
- 独立因果机制
  那么如何保证因果模型对联合分布分解是有意义的呢？在噪声Ui独立的情况下，根据因果图对联合分布进行因果分解(4)总是可行的，为此我们需要考虑与(4)中因子的独立机制。
- 一个系统的变量的因果生成过程是由一系列自主模块构成，它们不会影响彼此，也无法提供彼此的信息
- 微小的分布改变一般是稀疏地或局部地体现在因果分解(2)中，即它们通常不会同时影响所有的机制
  相反，如果我们考虑非因果的分解，例如(3)，那么大多数机制，即使不是全部会被同时影响。SMS假说近期已经被用来学习因果模型、模块化结构和解耦表征。
- 因果发现与机器学习
  上两部分介绍了因果模型，和一些必要的因果假设与因果机制，在这一部分我们讨论如何在上述的框架下学到因果模型，即因果发现。
- 在满足因果忠诚性假设的情况下，即不存在条件独立之外的独立性(这是为了保证独立性检验可以确定因果图中所有的边)，我们可以通过在观察数据中进行条件独立测试来发现因果图。但是这种方法存在一些问题：一是数据量总是有限的，而条件独立测试是很难的，尤其是在连续和高维的情况下，没有额外的假设，条件独立测试很难进行；二是条件独立测试可能会产生无法分辨边方向的马尔可夫等价类。
- 近年来通过假设结构方程的类型，我们发现这两个问题都可以被解决。下面的例子，可以很好地说明为什么假设SCM中的方程形式是必要的。考虑一个简单的SCM，只有两个变量X→Y，(1)可以写为
- X=U
- Y=f(X,V)
- X=U
- Y=f(X)+V
- 如果(4)中f对V是光滑的，且是相对集中的，那么(5)可以看做(4)的局部泰勒展开。
- 另外，对函数形式的限制不仅使得f的学习变得容易，还被发现可以打破双变量情况下的因果对称性，即可以确定马尔可夫等价类中边的方向。例如，如果加性噪声模型(5)中的f为非线性的，那么是不可以从相反的方向(即Y→X)来拟合出一个加性噪声模型的。具有这样性质的函数假设有许多，例如线性非高斯无环模型，后非线性模型等。
- 限制函数类型只是一种辨识因果结构的方法，其他可能性依然存在。根据SMS假说，不同环境的分布偏移可以极大的帮助我们辨识因果结构。这些环境可以来自于干预、不平稳的时间序列或者不同的视角。不变因果预测(Invariant Causal Prediction, ICP)框架就是考虑这样的情形。这些不同的环境可以理解为不同的任务，这可以联系到元学习(meta learning)。
- Bengio的工作就借助因果模型应该比一般的预测模型更快的适应干预这样的思想，把元学习中的泛化与因果模型的不变形绑定在一起。这项工作被推广到多变量与未知的干预的情形，并且通过把离散的图搜索转化为连续优化问题，提出了一个用神经网络进行因果发现的框架。也有人探索了如何用强化学习来学习因果模型。
- 上述的所有因果发现的方法，不论传统的还是结合机器学习的，都是在有语义的抽象表征已经给定的情形，不需要从高维的低层次的数据中学习。所以传统的因果发现算法无法应用在机器学习领域的数据上。若想利用因果推断来解决当前机器学习领域的难题，从非结构化数据中学习因果变量是一个绕不开的关键问题。
- 学习因果变量
  为此，我们尝试将因果变量S1,,,,Sn与观察量X通过因果表示学习联系起来
- 其中G是一个非线性函数。如图二所示，高维的观察量(图像)是由未知的因果系统的状态产生，然后我们希望用一个神经网络提取这些高层变量，以求在下游任务中取得更好的效果。在什么条件下我们可以找到这些可以用于因果模型的粗略变量呢？解答这个问题是很有挑战的。定义因果关联的对象或变量，可以被归结为估计这个世界的更细致的模型，例如微观结构方程模型、常微分方程、temporally aggregated time series等。定义可用于因果模型的单元，对人和机器都是有挑战的，这与现代机器学习领域试图学习鲁棒、可解释、公平的数据表征的目标是一致的。
- 我们应该尝试将SCM嵌入到等大的机器学习模型中，它的输入输出可能是高维非结构化的，但内部有一部分是由SCM决定的。这样的模型可能会是模块化的架构，不同的模块可以被微调或用于新的任务，SMS假说也可被用于学习合适的结构。
- 图三展示了，对因果变量稀疏的干预(改变个别变量)，会造成图像中稠密的变化(很多像素发生变化)，在有些情况下，例如改变光照或视角，会导致所有像素都变化。
- 接下来我们讨论考虑因果表征学习的三个机器学习问题。
- 问题一 ：学习解耦的表征
  如我们前文讨论的，ICM原则蕴含了(1)中SCM噪声项的独立性，并因此也蕴含了如下解耦表征的可行性
- 问题二：学习可迁移的机制
  一个人工或自然的智能能只能得到有限的资源和信息，这有关训练数据：与现代工业实践中的大规模人工标注相比，许多领域的数据都很少，所以需要池化活重复利用数据；也有关计算资源：动物的大脑尺寸是有限的，进化神经学表明在很多例子中，脑区可以用于不同的目的。在很多应用场景中，部署在嵌入式系统中的机器学习模型也面临这样的算力限制。所以未来的AI模型应该拥有动物一样，可以在鲁棒的解决一系列现实问题的能力，因此也应该应该有相似的可复用的组件。一个优雅的方法则是使用可以对应世界中模块化的模块化模型，换句话说，如果世界是模块化的，包括它的成份、机制，那么模型采用相应的模块是明智的。例如，在模式识别任务中，学习包含独立机制的因果模型，可以帮助模型在不同领域迁移。自我意识这样的生物功能，可能和在洛伦兹的想象空间中，表示自我的变量是相关的；而且自由意志可能将会成为这个自我变量与它所采取行动沟通的方式。
- 问题三：学习可被干预世界模型与推理
  因果表示学习应该要比传统的只关注统计相关性的表示学习更进一步，去学习支持干预、规划、推理的模型，实现康拉德·洛伦兹的“思考即为在想象中行动”的概念。这最终需要反思行为，想象可能的情况的能力，甚至可能需要自由意志(的幻觉)。这对社会和文化学习是至关重要的，是一个尚未登上机器学习领域舞台，但却是人类智能的核心。
- 对机器学习研究的启示
  上述的所有关于学习范式的讨论，都不基于常用的i.i.d.假设。因此，我们需要一个更弱的假设：模型将要被应用的数据是来自不同的分布，但设计几乎相同的因果机制。这会带来几个严肃的挑战：
- 我们需要从给定的低层次输入特征中抽象因果变量
  哪方面的数据可以揭示因果关系尚无共识
  传统的实验方案不足以推断和评估因果模型，我们需要新的基准测试
  即使是在我们了解的有限案例中，仍然缺乏可大规模使用的算法
  尽管如此，这样的努力对于机器学习有很多具体的影响。
- 半监督学习
  假定一个潜在的因果图为X→Y，且我们想学习X→Y的映射。在这种情况下，因果分解为
- P(X,Y)=P(X)P(Y|X)
- 根据ICM原理，P(X)不包含P(Y|X)任何的信息，这意味着半监督学习是无效的，即利用P(X)的额外信息(没有标签的数据)，是无法帮助我们提升P(Y|X=x)的估计的。即只有在相反(反因果)方向才可使用半监督学习。
- 对抗漏洞
  有人提出假说，因果方向应该对分类器是否易受对抗攻击也有影响。对机器学习系统的对抗攻击可以使用微小的人类不可见的扰动，改变分类器的输出。这与因果有下面几方面的联系。
- 首先，这些对抗攻击显然是违反了统计机器学习中i.i.d.的假设，所以模型出现错误是很自然的。这也揭示了，这些分类器所具有的鲁棒性与人类的是不同的。如果这两个鲁棒性的测度都已知，就可以在最小化一个时最大化另一个来找到攻击的手段。现在的对抗学习的方法可大致归结于此，把人的鲁棒性测度建模成一个数学上简单的集合，在这个集合中找到使分类器的输出发生最大变化的样本。
- 考虑到因果，最近有研究证实了：那么如果预测器是近似了内在的可迁移且鲁棒的因果机制，那么对抗样本应该是更难被找到的。所以结合因果，可以使机器学习模型更好的抵御对抗攻击。
- 鲁棒性与强泛化性
  承接上文，我们可以大胆的假设，有自主模块构成的结构，例如因果分解(1)，对换出或者求个单个组件更加鲁棒。而鲁棒性对于策略型行为同样重要，即那些需要考虑其他参与者行为的决策。例如，一个人可以通过例如搬到富裕的街区来改变银行对其信用的评级，对于银行来说，则只有用影响信用的真正原因变量构建模型，才能保证系统的鲁棒性。
- 预训练、数据增强与自监督
  通过求解(14)中的最小最大问题是困难的，我们现在介绍几种机器学习中常用的技术作为近似(14)的手段。
- 第一种是让训练集的分布更加丰富，即可以通过在海量和多样数据中进行预训练。只有当训练分布是足够丰富已涵盖其他分布的信息时才是有效的。
- 第二种方法常与上一种一起使用，数据增强，即通过特定人工生成的干预生成新数据来获得更加丰富的分布。例如图像中的旋转、翻转等。
- 第三种是通过自监督学习来学习P(X)。自监督学习即通过一些自动生成的无需人工标注的标签，把无监督学习转化为监督学习。这样的前置任务是为了迫使模型学习P(X)的信息，这会对下游任务起到帮助。而ICM原则可以帮助发展自动构建这些前置任务的方法。
- 强化学习
  强化学习是机器学习中更为接近因果研究的一个领域，它常常会直接有效的估计do-probabilities，即干预效果。例如on-policy学习就直接估计当前策略干预的do-probabilities。但当涉及off-policy学习或者离线强化学习时，与因果相关的问题就变得十分精妙。强化学习和因果结合的可以大致分为两类：一是在强化学习中结合因果发现，例如智能体可以学到环境的因果模型；二是学习利用因果模型进行规划和行动。已经有越来越多的证据表明，使用恰当的结构化表征是很有帮助的。
- 世界模型
  基于模型的强化学习是和因果紧密相连的，它期望学到一个环境的模型来代替与环境的交互，来解决目前强化学习的种种问题。所以利用因果推断的方法来帮助智能体学习一个更好的世界模型是很直观的想法。
- 泛化性、鲁棒性和更快的迁移性
  尽管强化学习已经取得了瞩目的成功，但它需要的样本量往往是现实应用中无法满足的。而且，强化学习智能体在面对一些训练时没有见过的微小的环境变化时是十分脆弱的。这在理论和实践上对这一领域的未来都是至关重要的。一种思路即为学习不同环境间的不变性。学习不变性的一个关键要求很可能是可以实施干预并从干预中学习。发展心理学认为，只有通过实验才能发现因果关系，这恰好可以建模为强化学习环境，智能体从中通过干预和观察效果来发现因果关系。另一方面，因果模型可以学到具有一系列独立因果机制的环境模型，即使有一些分布的变化，也无需全部推倒重来。当然，关于如何明确的定义与思考强化学习的泛化性，仍然有许多开放问题需要研究。
- 反事实推理
  反事实推理已经被证明可以提高强化学习算法的样本效率和性能。这与发展心理学是相符的，反思可以让我们推断过去行为的作用，以在未来可以根据目的选择相应的行为。所以未来强化学习的研究应该重点考虑反事实推理：智能体可以在想象空间中行动，构想假说，并用合适的干预来验证。
- 离线强化学习
  监督学习的成功很大程度上取决大量的标注的数据，而在强化学习中，从头收集大量高保真的多样数据在很多实际应用场景是昂贵甚至有伦理问题的。离线强化学习就致力于在已有的决策序列数据中学习，不需额外的实验或干预数据。有效利用观察数据可以使强化学习更加实用。为此，智能体需要推断与观察数据中不同的动作的效果，这实际上就是反事实推理的问题。另外，当前学习策略与收集数据策略的分布不匹配也会带来非i.i.d.的困难。所以，考虑不变形，将知识分解为独立因果机制可以对离线强化学习起到很大帮助。
- 科学应用
  在自然科学中应用机器学习时的一个基本问题是：在多大程度上，我们可以通过机器学习来补充我们对物理系统的理解。一个有序的领域是用神经网络进行物理仿真，大大地提高了效率。如果实验条件是精密控制的，OOD泛化能力往往不是必须的，尽管仿真器需要在实验条件变化是重新训练。
- 但另一方面，缺少系统性实验常常是其他应用领域的问题，例如医疗领域。如果我们想通过一系列数据建模病人的健康状况，并使用受控条件下医生的决策来训练一个诊断系统，那么在部署该系统是往往会失败。尽管在特定的决策中是有用的，但是理解抉择背后的因果机制是提供个性化和可靠的治疗建议所必须的。
- 因果对于理解医学现象也有很多潜在的帮助，例如，新冠疫情中，在例如辛普森悖论存在的情况下，因果中介分析可以就帮助区分各种因素对于病死率的不同效果。另一个例子是宇宙学，因果模型被用来在分析工具会造成混淆的情况下分辨系外行星。
- 多任务学习和持续学习
  当前的AI是狭隘的，即只能训练去在特定任务上使用。多任务学习就致力于构建可以在跨越不同环境的多个任务上工作的系统。这些任务通常拥有一些相同的特点，通过学习任务之间的相似性，系统可以在遇到新任务时高效的使用从先前任务中学到的知识。这种相似性的一种可能性，就是任务间共享的可以用因果模型建模的数据生成过程。
- 与此同时，我们已经在不考虑因果的情况下处理多任务问题很久了，并且取得了很多成果。于是一个关键的问题提出：“为什么我们不能训练一个巨大模型来学习环境的动力学，并且包含所有可能的干预呢？毕竟如果我们训练了大量的干预且我们期望神经网络可以在他们中泛化的话，分布式表征就可以泛化到没有见过的例子中”。对此，有以下几点需要考虑。首先，如果数据不够多样，最差情形误差总是会任意的高。尽管在短期内可以通过更大的模型和数据集来通过非OOD的基准测试，因果总是提供了一个重要的补充。模型的泛化性能是和它的假设(结构及如何训练)紧密相关的，而因果提供了更明确以及符合物理和人类认知的假设，例如ICM原则。如果这些假设是有效的，使用这些假设模型理应比没有使用的要好。更进一步，如果我们有一个对特定环境的所有干预都有效的模型，我们自然想要将它运用在动力学相似大不完全相同其他环境中。因果方法恰好指出了，这样组需要将知识分解为独立可重组的机制。所以我们应该在模块化机器学习或者其他遵循ICM原则的机器学习方法上做更多的工作。
- 独立同分布的机器学习只不过是一种数学抽象，而因果可能是大多数生命学习的根本。到目前为止，机器学习忽略了因果的整合，而实际上却可以获得诸多好处，将因果与深度学习结合很可能是通往多用途AI的必经之路。
- 总结
  本文讨论了描述世界的不同模型，包括因果模型和统计模型，以及这些模型在建模和数据方面的一系列假设。为努力将因果与机器学习领域相结合，文章首先介绍了因果推断的基本原理；然后讨论了独立因果机制及相关概念，例如不变性，可以为因果学习带来怎样的帮助；在此基础上，讨论因果变量已知的情况下，如何从观察及干预数据中学习因果关系；更进一步，提出了关于因果表示学习的开放问题；最后讨论了机器学习领域的问题可以如何更好的从因果的角度理解和解决。
- 基于本文的讨论，相信如下几个领域将会对未来的研究至关重要：
- 学习大规模非线性因果关系
  学习因果变量
  理解现存深度学习方法的归纳偏置
  学习正确建模世界因果的模型和智能体
  蔡心宇 | 作者
  龚鹤扬、陆超超 | 审校
  邓一雪 | 编辑
  ◆ ◆ ◆
- 搜索公众号：集智俱乐扫码关注集智俱乐部公众号
- 加入“没有围墙的研究所部